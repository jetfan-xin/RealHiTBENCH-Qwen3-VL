`torch_dtype` is deprecated! Use `dtype` instead!
Configuration:
  Model: /data/pan/4xin/models/Qwen3-VL-8B-Instruct
  Modality: mix
  Format: html
  Data path: /data/pan/4xin/datasets/RealHiTBench
  Use Flash Attention: True
  Multi-GPU Mode: Data Parallel (DataParallel)
  Generation Settings (Qwen3-VL Instruct Recommended):
    Temperature: 0.7
    Top-p: 0.8
    Top-k: 20
    Repetition Penalty: 1.0
    Presence Penalty: 1.5
    Max Tokens: 32768
  Batch size: 2

============================================================
BATCH INFERENCE MODE (batch_size=2)
============================================================
Loading Qwen3-VL model from /data/pan/4xin/models/Qwen3-VL-8B-Instruct...
Available GPUs: 1
Using DATA PARALLELISM (DataParallel) - all GPUs compute simultaneously
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 40.23it/s]
Model loaded with flash_attention_2 attention on cuda:0
Single GPU detected, DataParallel not needed
Processor configured with dynamic resolution: min_pixels=200704, max_pixels=1605632
Successfully loaded F1, EM, ROUGE, SacreBLEU
Loaded QA file: QA_final_sc_filled.json (3071 queries)
Filtered to 396 queries of type: Structure Comprehending
Pending queries after filtering: 396
Processing batches:   0%|          | 0/198 [00:00<?, ?it/s]Processing batches:   1%|          | 1/198 [00:08<27:05,  8.25s/it]Processing batches:   1%|          | 2/198 [00:12<19:23,  5.93s/it]Processing batches:   2%|▏         | 3/198 [00:17<17:44,  5.46s/it]Processing batches:   2%|▏         | 4/198 [00:22<16:39,  5.15s/it]Processing batches:   3%|▎         | 5/198 [00:27<16:15,  5.05s/it]Processing batches:   3%|▎         | 6/198 [00:31<15:27,  4.83s/it]Processing batches:   4%|▎         | 7/198 [00:36<15:23,  4.83s/it]Processing batches:   4%|▍         | 8/198 [00:38<13:09,  4.15s/it]Processing batches:   5%|▍         | 9/198 [00:42<12:04,  3.83s/it]Processing batches:   5%|▌         | 10/198 [00:45<11:40,  3.73s/it]Processing batches:   6%|▌         | 11/198 [00:49<11:20,  3.64s/it]Processing batches:   6%|▌         | 12/198 [00:52<11:04,  3.57s/it]Processing batches:   7%|▋         | 13/198 [00:56<11:50,  3.84s/it]
============================================================
Batch 1/198: Processing 2 queries
============================================================
Predictions:  ['164287', '2023 2022']
Answer:  ['164287', '2023']
Predictions:  ['164287']
Answer:  ['164287']
  [Batch] Query 2676: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (4.01s avg)
Predictions:  ['2023 2022']
Answer:  ['2023']
  [Batch] Query 2677: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (4.01s avg)
Checkpoint saved: 2 queries processed

============================================================
Batch 2/198: Processing 2 queries
============================================================
Predictions:  ['2829', 'yes']
Answer:  ['2829', 'yes']
Predictions:  ['2829']
Answer:  ['2829']
  [Batch] Query 2678: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.04s avg)
Predictions:  ['yes']
Answer:  ['yes']
  [Batch] Query 2679: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.04s avg)
Checkpoint saved: 4 queries processed

============================================================
Batch 3/198: Processing 2 queries
============================================================
Predictions:  ['38489 24263', 'bachelors degree only associate degree']
Answer:  ['39357', 'associate degree']
Predictions:  ['38489 24263']
Answer:  ['39357']
  [Batch] Query 2680: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.33s avg)
Predictions:  ['bachelors degree only associate degree']
Answer:  ['associate degree']
  [Batch] Query 2681: {'F1': 57.14, 'EM': 0.0, 'ROUGE-L': 57.14, 'SacreBLEU': 21.36} (2.33s avg)
Checkpoint saved: 6 queries processed

============================================================
Batch 4/198: Processing 2 queries
============================================================
Predictions:  ['7797 655', 'women men']
Answer:  ['7668', 'women']
Predictions:  ['7797 655']
Answer:  ['7668']
  [Batch] Query 2682: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.22s avg)
Predictions:  ['women men']
Answer:  ['women']
  [Batch] Query 2683: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.22s avg)
Checkpoint saved: 8 queries processed

============================================================
Batch 5/198: Processing 2 queries
============================================================
Predictions:  ['5243', 'white black']
Answer:  ['5243', 'white']
Predictions:  ['5243']
Answer:  ['5243']
  [Batch] Query 2684: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.32s avg)
Predictions:  ['white black']
Answer:  ['white']
  [Batch] Query 2685: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.32s avg)
Checkpoint saved: 10 queries processed

============================================================
Batch 6/198: Processing 2 queries
============================================================
Predictions:  ['17', '37 16']
Answer:  ['17', '65']
Predictions:  ['17']
Answer:  ['17']
  [Batch] Query 2686: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.08s avg)
Predictions:  ['37 16']
Answer:  ['65']
  [Batch] Query 2687: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.08s avg)
Checkpoint saved: 12 queries processed

============================================================
Batch 7/198: Processing 2 queries
============================================================
Predictions:  ['35', '128154 21538']
Answer:  ['35', '128154 21538']
Predictions:  ['35']
Answer:  ['35']
  [Batch] Query 2688: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.30s avg)
Predictions:  ['128154 21538']
Answer:  ['128154 21538']
  [Batch] Query 2689: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.30s avg)
Checkpoint saved: 14 queries processed

============================================================
Batch 8/198: Processing 2 queries
============================================================
Predictions:  ['587 34282', '134']
Answer:  ['33695', '134']
Predictions:  ['587 34282']
Answer:  ['33695']
  [Batch] Query 2690: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.23s avg)
Predictions:  ['134']
Answer:  ['134']
  [Batch] Query 2691: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.23s avg)
Checkpoint saved: 16 queries processed

============================================================
Batch 9/198: Processing 2 queries
============================================================
Predictions:  ['35 hours and over', '13342 8674']
Answer:  ['35 hours and over', '4826']
Predictions:  ['35 hours and over']
Answer:  ['35 hours and over']
  [Batch] Query 2692: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (1.45s avg)
Predictions:  ['13342 8674']
Answer:  ['4826']
  [Batch] Query 2693: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.45s avg)
Checkpoint saved: 18 queries processed

============================================================
Batch 10/198: Processing 2 queries
============================================================
Predictions:  ['manufacturing wholesale and retail trade education and health services leisure and hospitality', 'leisure and hospitality wholesale and retail trade education and health services']
Answer:  ['7', 'mining quarrying and oil and gas extraction unpaid family workers information']
Predictions:  ['manufacturing wholesale and retail trade education and health services leisure and hospitality']
Answer:  ['7']
  [Batch] Query 2694: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.57s avg)
Predictions:  ['leisure and hospitality wholesale and retail trade education and health services']
Answer:  ['mining quarrying and oil and gas extraction unpaid family workers information']
  [Batch] Query 2695: {'F1': 18.18, 'EM': 0.0, 'ROUGE-L': 18.18, 'SacreBLEU': 4.46} (1.57s avg)
Checkpoint saved: 20 queries processed

============================================================
Batch 11/198: Processing 2 queries
============================================================
Predictions:  ['31', 'men 06']
Answer:  ['31', 'men 01']
Predictions:  ['31']
Answer:  ['31']
  [Batch] Query 2696: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.59s avg)
Predictions:  ['men 06']
Answer:  ['men 01']
  [Batch] Query 2697: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (1.59s avg)
Checkpoint saved: 22 queries processed

============================================================
Batch 12/198: Processing 2 queries
============================================================
Predictions:  ['20', '1177']
Answer:  ['20', '1217']
Predictions:  ['20']
Answer:  ['20']
  [Batch] Query 2698: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.59s avg)
Predictions:  ['1177']
Answer:  ['1217']
  [Batch] Query 2699: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.59s avg)
Checkpoint saved: 24 queries processed

============================================================
Batch 13/198: Processing 2 queries
============================================================
Predictions:  ['304', 'falls slips trips exposure to harmful substances or environments fires and explosions']
Answer:  ['304', 'falls slips trips exposure to harmful substances or environments fires and explosions']
Predictions:  ['304']
Answer:  ['304']
  [Batch] Query 2700: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.90s avg)
Predictions:  ['falls slips trips exposure to harmful substances or environments fires and explosions']
Answer:  ['falls slips trips exposure to harmful substances or environments fires and explosions']
  [Batch] Query 2701: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (1.90s avg)
Checkpoint saved: 26 queries processed

============================================================
Batch 14/198: Processing 2 queries
============================================================
Predictions:  ['81 90', '01 02']
Answer:  ['162', '07']
Predictions:  ['81 90']
Answer:  ['162']
Processing batches:   7%|▋         | 14/198 [01:14<24:20,  7.94s/it]Processing batches:   8%|▊         | 15/198 [01:35<35:59, 11.80s/it]Processing batches:   8%|▊         | 16/198 [01:47<36:32, 12.05s/it]Processing batches:   9%|▊         | 17/198 [01:51<29:14,  9.70s/it]Processing batches:   9%|▉         | 18/198 [02:20<45:52, 15.29s/it]Processing batches:  10%|▉         | 19/198 [02:46<55:43, 18.68s/it]Processing batches:  10%|█         | 20/198 [03:18<1:07:15, 22.67s/it]Processing batches:  11%|█         | 21/198 [03:42<1:07:24, 22.85s/it]Processing batches:  11%|█         | 22/198 [04:06<1:08:47, 23.45s/it]Processing batches:  12%|█▏        | 23/198 [04:11<51:31, 17.66s/it]  Processing batches:  12%|█▏        | 24/198 [04:15<39:19, 13.56s/it]Processing batches:  13%|█▎        | 25/198 [04:20<32:16, 11.19s/it]Processing batches:  13%|█▎        | 26/198 [04:26<27:13,  9.50s/it]  [Batch] Query 2702: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (8.59s avg)
Predictions:  ['01 02']
Answer:  ['07']
  [Batch] Query 2703: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (8.59s avg)
Checkpoint saved: 28 queries processed

============================================================
Batch 15/198: Processing 2 queries
============================================================
Predictions:  ['08 08', '18 23 35 12 23']
Answer:  ['fulltime employed mothers 03', '18']
Predictions:  ['08 08']
Answer:  ['fulltime employed mothers 03']
  [Batch] Query 2704: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (10.03s avg)
Predictions:  ['18 23 35 12 23']
Answer:  ['18']
  [Batch] Query 2705: {'F1': 33.33, 'EM': 0.0, 'ROUGE-L': 33.33, 'SacreBLEU': 10.68} (10.03s avg)
Checkpoint saved: 30 queries processed

============================================================
Batch 16/198: Processing 2 queries
============================================================
Predictions:  ['not employed', '13']
Answer:  ['not employed', '01']
Predictions:  ['not employed']
Answer:  ['not employed']
  [Batch] Query 2706: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (6.19s avg)
Predictions:  ['13']
Answer:  ['01']
  [Batch] Query 2707: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (6.19s avg)
Checkpoint saved: 32 queries processed

============================================================
Batch 17/198: Processing 2 queries
============================================================
Predictions:  ['85 81', '51']
Answer:  ['employed parttime married mothers', '41']
Predictions:  ['85 81']
Answer:  ['employed parttime married mothers']
  [Batch] Query 2708: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.99s avg)
Predictions:  ['51']
Answer:  ['41']
  [Batch] Query 2709: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.99s avg)
Checkpoint saved: 34 queries processed

============================================================
Batch 18/198: Processing 2 queries
============================================================
Predictions:  ['29 37', 'caring for and helping household children physical care']
Answer:  ['29 37', 'household activities']
Predictions:  ['29 37']
Answer:  ['29 37']
  [Batch] Query 2710: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (14.03s avg)
Predictions:  ['caring for and helping household children physical care']
Answer:  ['household activities']
  [Batch] Query 2711: {'F1': 20.0, 'EM': 0.0, 'ROUGE-L': 20.0, 'SacreBLEU': 5.52} (14.03s avg)
Checkpoint saved: 36 queries processed

============================================================
Batch 19/198: Processing 2 queries
============================================================
Predictions:  ['playingdoing hobbies with children', '27 14']
Answer:  ['reading towith children', '22']
Predictions:  ['playingdoing hobbies with children']
Answer:  ['reading towith children']
  [Batch] Query 2712: {'F1': 28.57, 'EM': 0.0, 'ROUGE-L': 28.57, 'SacreBLEU': 15.97} (13.16s avg)
Predictions:  ['27 14']
Answer:  ['22']
  [Batch] Query 2713: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (13.16s avg)
Checkpoint saved: 38 queries processed

============================================================
Batch 20/198: Processing 2 queries
============================================================
Predictions:  ['sleeping personal care activities household activities caring for and helping household members caring for and helping household children physical care educationrelated activities reading towith children playingdoing hobbies with children', 'travel related to caring for and helping household children']
Answer:  ['watching television', 'reading towith children']
Predictions:  ['sleeping personal care activities household activities caring for and helping household members caring for and helping household children physical care educationrelated activities reading towith children playingdoing hobbies with children']
Answer:  ['watching television']
  [Batch] Query 2714: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (15.87s avg)
Predictions:  ['travel related to caring for and helping household children']
Answer:  ['reading towith children']
  [Batch] Query 2715: {'F1': 16.67, 'EM': 0.0, 'ROUGE-L': 16.67, 'SacreBLEU': 4.77} (15.87s avg)
Checkpoint saved: 40 queries processed

============================================================
Batch 21/198: Processing 2 queries
============================================================
Predictions:  ['07 07', '05 05']
Answer:  ['05', '06']
Predictions:  ['07 07']
Answer:  ['05']
  [Batch] Query 2716: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (11.51s avg)
Predictions:  ['05 05']
Answer:  ['06']
  [Batch] Query 2717: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (11.51s avg)
Checkpoint saved: 42 queries processed

============================================================
Batch 22/198: Processing 2 queries
============================================================
Predictions:  ['mother employed part time and father employed full time mother not employed and father employed full time', '07']
Answer:  ['3', '06']
Predictions:  ['mother employed part time and father employed full time mother not employed and father employed full time']
Answer:  ['3']
  [Batch] Query 2718: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (12.31s avg)
Predictions:  ['07']
Answer:  ['06']
  [Batch] Query 2719: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (12.31s avg)
Checkpoint saved: 44 queries processed

============================================================
Batch 23/198: Processing 2 queries
============================================================
Predictions:  ['middle atlantic', '56 56']
Answer:  ['pacific', '84']
Predictions:  ['middle atlantic']
Answer:  ['pacific']
  [Batch] Query 2720: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.52s avg)
Predictions:  ['56 56']
Answer:  ['84']
  [Batch] Query 2721: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.52s avg)
Checkpoint saved: 46 queries processed

============================================================
Batch 24/198: Processing 2 queries
============================================================
Predictions:  ['7', 'service occupations']
Answer:  ['7', 'service occupations']
Predictions:  ['7']
Answer:  ['7']
  [Batch] Query 2722: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.83s avg)
Predictions:  ['service occupations']
Answer:  ['service occupations']
  [Batch] Query 2723: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.83s avg)
Checkpoint saved: 48 queries processed

============================================================
Batch 25/198: Processing 2 queries
============================================================
Predictions:  ['public administration', '12171 12484']
Answer:  ['transportation and material moving', '246551']
Predictions:  ['public administration']
Answer:  ['transportation and material moving']
  [Batch] Query 2724: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.72s avg)
Predictions:  ['12171 12484']
Answer:  ['246551']
  [Batch] Query 2725: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.72s avg)
Checkpoint saved: 50 queries processed

============================================================
Batch 26/198: Processing 2 queries
============================================================
Predictions:  ['3', '182527']
Answer:  ['26', '1825266']
Predictions:  ['3']
Answer:  ['26']
  [Batch] Query 2726: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.65s avg)
Predictions:  ['182527']
Answer:  ['1825266']
  [Batch] Query 2727: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.65s avg)
Checkpoint saved: 52 queries processed

============================================================
Batch 27/198: Processing 2 queries
============================================================
Predictions:  ['12171 12484', '35984976 37797143']
Answer:  ['123275', '599694862']
Predictions:  Processing batches:  14%|█▎        | 27/198 [04:32<24:26,  8.58s/it]Processing batches:  14%|█▍        | 28/198 [04:38<22:18,  7.87s/it]Processing batches:  15%|█▍        | 29/198 [04:43<19:37,  6.97s/it]Processing batches:  15%|█▌        | 30/198 [04:48<17:39,  6.30s/it]Processing batches:  16%|█▌        | 31/198 [04:51<15:10,  5.45s/it]Processing batches:  16%|█▌        | 32/198 [04:55<13:24,  4.85s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1223, in forward
    outputs = self.language_model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 850, in forward
    layer_outputs = decoder_layer(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 517, in forward
    hidden_states = self.mlp(hidden_states)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 472, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.37 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 76.89 GiB memory in use. Of the allocated memory 51.56 GiB is allocated by PyTorch, and 24.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  17%|█▋        | 33/198 [05:15<25:41,  9.34s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1223, in forward
    outputs = self.language_model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 850, in forward
    layer_outputs = decoder_layer(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 517, in forward
    hidden_states = self.mlp(hidden_states)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 472, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.37 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.22 GiB is free. Including non-PyTorch memory, this process has 74.02 GiB memory in use. Of the allocated memory 50.16 GiB is allocated by PyTorch, and 23.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  17%|█▋        | 34/198 [05:35<34:49, 12.74s/it]Processing batches:  18%|█▊        | 35/198 [05:46<32:28, 11.95s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (335725 > 262144). Running this sequence through the model will result in indexing errors
Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1223, in forward
    outputs = self.language_model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 850, in forward
    layer_outputs = decoder_layer(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 517, in forward
    hidden_states = self.mlp(hidden_states)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 472, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/activations.py", line 99, in forward
    return nn.functional.silu(input)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
    return torch._C._nn.silu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.37 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 50.17 GiB is allocated by PyTorch, and 26.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  18%|█▊        | 36/198 [05:57<31:29, 11.67s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1223, in forward
    outputs = self.language_model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 850, in forward
    layer_outputs = decoder_layer(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 517, in forward
    hidden_states = self.mlp(hidden_states)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 472, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/activations.py", line 99, in forward
    return nn.functional.silu(input)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
    return torch._C._nn.silu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.37 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 50.17 GiB is allocated by PyTorch, and 26.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  19%|█▊        | 37/198 [06:00<24:26,  9.11s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1223, in forward
    outputs = self.language_model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 850, in forward
    layer_outputs = decoder_layer(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 517, in forward
    hidden_states = self.mlp(hidden_states)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 472, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.14 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.10 GiB is free. Including non-PyTorch memory, this process has 76.14 GiB memory in use. Of the allocated memory 40.48 GiB is allocated by PyTorch, and 35.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  19%|█▉        | 38/198 [06:15<29:08, 10.93s/it]['12171 12484']
Answer:  ['123275']
  [Batch] Query 2728: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.09s avg)
Predictions:  ['35984976 37797143']
Answer:  ['599694862']
  [Batch] Query 2729: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.09s avg)
Checkpoint saved: 54 queries processed

============================================================
Batch 28/198: Processing 2 queries
============================================================
Predictions:  ['china european union27 mexico japan indonesia taiwan egypt south korea vietnam thailand', 'aug 2022 1170911']
Answer:  ['china european union27 mexico japan indonesia taiwan egypt south korea vietnam thailand', 'aug 2022 9634024']
Predictions:  ['china european union27 mexico japan indonesia taiwan egypt south korea vietnam thailand']
Answer:  ['china european union27 mexico japan indonesia taiwan egypt south korea vietnam thailand']
  [Batch] Query 2730: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (2.99s avg)
Predictions:  ['aug 2022 1170911']
Answer:  ['aug 2022 9634024']
  [Batch] Query 2731: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.99s avg)
Checkpoint saved: 56 queries processed

============================================================
Batch 29/198: Processing 2 queries
============================================================
Predictions:  ['no', '3902']
Answer:  ['no', '3902']
Predictions:  ['no']
Answer:  ['no']
  [Batch] Query 2732: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.31s avg)
Predictions:  ['3902']
Answer:  ['3902']
  [Batch] Query 2733: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.31s avg)
Checkpoint saved: 58 queries processed

============================================================
Batch 30/198: Processing 2 queries
============================================================
Predictions:  ['2021 2022 2023', '100']
Answer:  ['2021 2022', '172']
Predictions:  ['2021 2022 2023']
Answer:  ['2021 2022']
  [Batch] Query 2734: {'F1': 80.0, 'EM': 0.0, 'ROUGE-L': 80.0, 'SacreBLEU': 0.0} (2.26s avg)
Predictions:  ['100']
Answer:  ['172']
  [Batch] Query 2735: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.26s avg)
Checkpoint saved: 60 queries processed

============================================================
Batch 31/198: Processing 2 queries
============================================================
Predictions:  ['5 mgl malathion 0 mgl malathion', '5 mgl malathion 1 mgl malathion 0 mgl malathion']
Answer:  ['5 mgl', '1 mgl 5 mgl 0 mgl']
Predictions:  ['5 mgl malathion 0 mgl malathion']
Answer:  ['5 mgl']
  [Batch] Query 2736: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 16.23} (1.61s avg)
Predictions:  ['5 mgl malathion 1 mgl malathion 0 mgl malathion']
Answer:  ['1 mgl 5 mgl 0 mgl']
  [Batch] Query 2737: {'F1': 80.0, 'EM': 0.0, 'ROUGE-L': 53.33, 'SacreBLEU': 16.52} (1.61s avg)
Checkpoint saved: 62 queries processed

============================================================
Batch 32/198: Processing 2 queries
============================================================
Predictions:  ['england', '2681686']
Answer:  ['england', '8938953']
Predictions:  ['england']
Answer:  ['england']
  [Batch] Query 2738: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.60s avg)
Predictions:  ['2681686']
Answer:  ['8938953']
  [Batch] Query 2739: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.60s avg)
Checkpoint saved: 64 queries processed

============================================================
Batch 33/198: Processing 2 queries
============================================================
Error processing batch 33: CUDA out of memory. Tried to allocate 8.37 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 76.89 GiB memory in use. Of the allocated memory 51.56 GiB is allocated by PyTorch, and 24.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 34/198: Processing 2 queries
============================================================
Error processing batch 34: CUDA out of memory. Tried to allocate 8.37 GiB. GPU 0 has a total capacity of 79.25 GiB of which 5.22 GiB is free. Including non-PyTorch memory, this process has 74.02 GiB memory in use. Of the allocated memory 50.16 GiB is allocated by PyTorch, and 23.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 35/198: Processing 2 queries
============================================================
Predictions:  ['£3173375 £3302667', 'creative']
Answer:  ['64760422', 'creative']
Predictions:  ['£3173375 £3302667']
Answer:  ['64760422']
  [Batch] Query 2744: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (4.91s avg)
Predictions:  ['creative']
Answer:  ['creative']
  [Batch] Query 2745: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (4.91s avg)
Checkpoint saved: 66 queries processed

============================================================
Batch 36/198: Processing 2 queries
============================================================
Error processing batch 36: CUDA out of memory. Tried to allocate 15.37 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 50.17 GiB is allocated by PyTorch, and 26.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 37/198: Processing 2 queries
============================================================
Error processing batch 37: CUDA out of memory. Tried to allocate 15.37 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 50.17 GiB is allocated by PyTorch, and 26.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 38/198: Processing 2 queries
============================================================
Error processing batch 38: CUDA out of memory. Tried to allocate 20.14 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.10 GiB is free. Including non-PyTorch memory, this process has 76.14 GiB memory in use. Of the allocated memory 40.48 GiB is allocated by PyTorch, and 35.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 39/198: Processing 2 queries
============================================================
Predictions:  ['192617', 'male female']
Answer:  ['192617', 'female']
Predictions:  ['192617']
Answer:  ['192617']
  [Batch] Query 2752: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (12.43s avg)
Predictions:  ['male female']
Answer:  ['female']
Processing batches:  20%|█▉        | 39/198 [06:40<40:13, 15.18s/it]Processing batches:  20%|██        | 40/198 [06:47<33:56, 12.89s/it]Processing batches:  21%|██        | 41/198 [06:56<30:14, 11.56s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1143, in forward
    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.24 GiB. GPU 0 has a total capacity of 79.25 GiB of which 38.33 GiB is free. Including non-PyTorch memory, this process has 40.91 GiB memory in use. Of the allocated memory 36.67 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  21%|██        | 42/198 [07:01<25:10,  9.68s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1143, in forward
    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.24 GiB. GPU 0 has a total capacity of 79.25 GiB of which 38.33 GiB is free. Including non-PyTorch memory, this process has 40.91 GiB memory in use. Of the allocated memory 36.68 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  22%|██▏       | 43/198 [07:07<21:47,  8.44s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1143, in forward
    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.81 GiB. GPU 0 has a total capacity of 79.25 GiB of which 38.33 GiB is free. Including non-PyTorch memory, this process has 40.91 GiB memory in use. Of the allocated memory 35.96 GiB is allocated by PyTorch, and 4.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  22%|██▏       | 44/198 [07:12<19:24,  7.56s/it]Processing batches:  23%|██▎       | 45/198 [07:17<16:44,  6.57s/it]Processing batches:  23%|██▎       | 46/198 [07:21<15:07,  5.97s/it]Processing batches:  24%|██▎       | 47/198 [07:24<13:02,  5.18s/it]Processing batches:  24%|██▍       | 48/198 [07:29<12:50,  5.14s/it]Processing batches:  25%|██▍       | 49/198 [07:35<13:03,  5.26s/it]Processing batches:  25%|██▌       | 50/198 [07:39<12:02,  4.88s/it]Processing batches:  26%|██▌       | 51/198 [07:48<14:52,  6.07s/it]  [Batch] Query 2753: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (12.43s avg)
Checkpoint saved: 68 queries processed

============================================================
Batch 40/198: Processing 2 queries
============================================================
Predictions:  ['no male', '2926320']
Answer:  ['no female', '2926310']
Predictions:  ['no male']
Answer:  ['no female']
  [Batch] Query 2754: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (3.65s avg)
Predictions:  ['2926320']
Answer:  ['2926310']
  [Batch] Query 2755: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.65s avg)
Checkpoint saved: 70 queries processed

============================================================
Batch 41/198: Processing 2 queries
============================================================
Predictions:  ['2017 1667430', '1612160']
Answer:  ['2020 1861550', '1612160']
Predictions:  ['2017 1667430']
Answer:  ['2020 1861550']
  [Batch] Query 2756: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (4.11s avg)
Predictions:  ['1612160']
Answer:  ['1612160']
  [Batch] Query 2757: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (4.11s avg)
Checkpoint saved: 72 queries processed

============================================================
Batch 42/198: Processing 2 queries
============================================================
Error processing batch 42: CUDA out of memory. Tried to allocate 40.24 GiB. GPU 0 has a total capacity of 79.25 GiB of which 38.33 GiB is free. Including non-PyTorch memory, this process has 40.91 GiB memory in use. Of the allocated memory 36.67 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 43/198: Processing 2 queries
============================================================
Error processing batch 43: CUDA out of memory. Tried to allocate 40.24 GiB. GPU 0 has a total capacity of 79.25 GiB of which 38.33 GiB is free. Including non-PyTorch memory, this process has 40.91 GiB memory in use. Of the allocated memory 36.68 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 44/198: Processing 2 queries
============================================================
Error processing batch 44: CUDA out of memory. Tried to allocate 38.81 GiB. GPU 0 has a total capacity of 79.25 GiB of which 38.33 GiB is free. Including non-PyTorch memory, this process has 40.91 GiB memory in use. Of the allocated memory 35.96 GiB is allocated by PyTorch, and 4.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 45/198: Processing 2 queries
============================================================
Predictions:  ['arsenal newcastle', '37']
Answer:  ['arsenal newcastle', '43']
Predictions:  ['arsenal newcastle']
Answer:  ['arsenal newcastle']
  [Batch] Query 2764: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.00s avg)
Predictions:  ['37']
Answer:  ['43']
  [Batch] Query 2765: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.00s avg)
Checkpoint saved: 74 queries processed

============================================================
Batch 46/198: Processing 2 queries
============================================================
Predictions:  ['away home', '30 122']
Answer:  ['away', '152']
Predictions:  ['away home']
Answer:  ['away']
  [Batch] Query 2766: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.17s avg)
Predictions:  ['30 122']
Answer:  ['152']
  [Batch] Query 2767: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.17s avg)
Checkpoint saved: 76 queries processed

============================================================
Batch 47/198: Processing 2 queries
============================================================
Predictions:  ['butter m eggs', 'tea 00']
Answer:  ['eggs', 'butter t 00']
Predictions:  ['butter m eggs']
Answer:  ['eggs']
  [Batch] Query 2768: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (1.55s avg)
Predictions:  ['tea 00']
Answer:  ['butter t 00']
  [Batch] Query 2769: {'F1': 40.0, 'EM': 0.0, 'ROUGE-L': 40.0, 'SacreBLEU': 0.0} (1.55s avg)
Checkpoint saved: 78 queries processed

============================================================
Batch 48/198: Processing 2 queries
============================================================
Predictions:  ['2283', '578 eclaire 373 diamo']
Answer:  ['2661', '578 eclaire']
Predictions:  ['2283']
Answer:  ['2661']
  [Batch] Query 2770: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.40s avg)
Predictions:  ['578 eclaire 373 diamo']
Answer:  ['578 eclaire']
  [Batch] Query 2771: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 31.95} (2.40s avg)
Checkpoint saved: 80 queries processed

============================================================
Batch 49/198: Processing 2 queries
============================================================
Predictions:  ['373 diamo 597moisant 433 mosaic meemo sparrow', '51492']
Answer:  ['2', '51492']
Predictions:  ['373 diamo 597moisant 433 mosaic meemo sparrow']
Answer:  ['2']
  [Batch] Query 2772: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.65s avg)
Predictions:  ['51492']
Answer:  ['51492']
  [Batch] Query 2773: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.65s avg)
Checkpoint saved: 82 queries processed

============================================================
Batch 50/198: Processing 2 queries
============================================================
Predictions:  ['product 1 product 3 product 8 product 9 product 10', 'no']
Answer:  ['product 2 product 3 product 4 product 5 product 6 product 9', 'false']
Predictions:  ['product 1 product 3 product 8 product 9 product 10']
Answer:  ['product 2 product 3 product 4 product 5 product 6 product 9']
  [Batch] Query 2774: {'F1': 63.64, 'EM': 0.0, 'ROUGE-L': 54.55, 'SacreBLEU': 17.49} (1.89s avg)
Predictions:  ['no']
Answer:  ['false']
  [Batch] Query 2775: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.89s avg)
Checkpoint saved: 84 queries processed

============================================================
Batch 51/198: Processing 2 queries
============================================================
Predictions:  ['product 2 product 8 product 10 product 3 product 4 product 9 product 6 product 7 product 5 product 1', 'john cross 2']
Answer:  ['product 2 product 8 product 5 product 7 product 10 product 4 product 9 product 1 product 3 product 6', 'john cross 2']
Predictions:  ['product 2 product 8 product 10 product 3 product 4 product 9 product 6 product 7 product 5 product 1']
Answer:  ['product 2 product 8 product 5 product 7 product 10 product 4 product 9 product 1 product 3 product 6']
  [Batch] Query 2776: {'F1': 100.0, 'EM': 0.0, 'ROUGE-L': 65.0, 'SacreBLEU': 59.32} (4.30s avg)
Predictions:  ['john cross 2']
Answer:  ['john cross 2']
  [Batch] Query 2777: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (4.30s avg)
Checkpoint saved: 86 queries processed

============================================================
Batch 52/198: Processing 2 queries
============================================================
Predictions:  Processing batches:  26%|██▋       | 52/198 [08:04<22:09,  9.11s/it]Processing batches:  27%|██▋       | 53/198 [08:10<19:43,  8.16s/it]Processing batches:  27%|██▋       | 54/198 [08:12<15:15,  6.36s/it]Processing batches:  28%|██▊       | 55/198 [08:15<12:18,  5.16s/it]Processing batches:  28%|██▊       | 56/198 [08:17<10:24,  4.40s/it]Processing batches:  29%|██▉       | 57/198 [08:22<10:19,  4.39s/it]Processing batches:  29%|██▉       | 58/198 [08:25<09:42,  4.16s/it]Processing batches:  30%|██▉       | 59/198 [08:29<09:35,  4.14s/it]Processing batches:  30%|███       | 60/198 [08:34<09:37,  4.18s/it]Processing batches:  31%|███       | 61/198 [08:38<09:26,  4.14s/it]Processing batches:  31%|███▏      | 62/198 [08:40<08:24,  3.71s/it]Processing batches:  32%|███▏      | 63/198 [08:47<10:37,  4.72s/it]Processing batches:  32%|███▏      | 64/198 [08:56<13:03,  5.85s/it]['18 8', 'john cross carl mackinson barry travis mark davies marc hilton dave griffiths jack cross mick tootle brian griffiths thomas evans james dake jack davies dean richards kevin riley daniel hudspeth callum ashcroft jeff lynch mark evans kev thompson']
Answer:  ['4', '5']
Predictions:  ['18 8']
Answer:  ['4']
  [Batch] Query 2778: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (7.98s avg)
Predictions:  ['john cross carl mackinson barry travis mark davies marc hilton dave griffiths jack cross mick tootle brian griffiths thomas evans james dake jack davies dean richards kevin riley daniel hudspeth callum ashcroft jeff lynch mark evans kev thompson']
Answer:  ['5']
  [Batch] Query 2779: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (7.98s avg)
Checkpoint saved: 88 queries processed

============================================================
Batch 53/198: Processing 2 queries
============================================================
Predictions:  ['2 0', 'rome']
Answer:  ['2', 'cairo']
Predictions:  ['2 0']
Answer:  ['2']
  [Batch] Query 2780: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.86s avg)
Predictions:  ['rome']
Answer:  ['cairo']
  [Batch] Query 2781: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.86s avg)
Checkpoint saved: 90 queries processed

============================================================
Batch 54/198: Processing 2 queries
============================================================
Predictions:  ['yes', 'taylor drew']
Answer:  ['yes', 'drew']
Predictions:  ['yes']
Answer:  ['yes']
  [Batch] Query 2782: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (0.96s avg)
Predictions:  ['taylor drew']
Answer:  ['drew']
  [Batch] Query 2783: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (0.96s avg)
Checkpoint saved: 92 queries processed

============================================================
Batch 55/198: Processing 2 queries
============================================================
Predictions:  ['taylor morgan drew', 'yes 250']
Answer:  ['taylor drew morgan', 'yes 250']
Predictions:  ['taylor morgan drew']
Answer:  ['taylor drew morgan']
  [Batch] Query 2784: {'F1': 100.0, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (1.07s avg)
Predictions:  ['yes 250']
Answer:  ['yes 250']
  [Batch] Query 2785: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.07s avg)
Checkpoint saved: 94 queries processed

============================================================
Batch 56/198: Processing 2 queries
============================================================
Predictions:  ['cpp7892531 11000', 'qr9785666 2235']
Answer:  ['cpp7892531 11000', 'qr9785666 2235']
Predictions:  ['cpp7892531 11000']
Answer:  ['cpp7892531 11000']
  [Batch] Query 2786: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.19s avg)
Predictions:  ['qr9785666 2235']
Answer:  ['qr9785666 2235']
  [Batch] Query 2787: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.19s avg)
Checkpoint saved: 96 queries processed

============================================================
Batch 57/198: Processing 2 queries
============================================================
Predictions:  ['cpp7892531 kp78952 glp0111234', 'yes']
Answer:  ['cpp7892531 kp78952 glp0111234', 'yes']
Predictions:  ['cpp7892531 kp78952 glp0111234']
Answer:  ['cpp7892531 kp78952 glp0111234']
  [Batch] Query 2788: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.08s avg)
Predictions:  ['yes']
Answer:  ['yes']
  [Batch] Query 2789: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.08s avg)
Checkpoint saved: 98 queries processed

============================================================
Batch 58/198: Processing 2 queries
============================================================
Predictions:  ['yes', '2 32 2']
Answer:  ['no', '68']
Predictions:  ['yes']
Answer:  ['no']
  [Batch] Query 2790: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.70s avg)
Predictions:  ['2 32 2']
Answer:  ['68']
  [Batch] Query 2791: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.70s avg)
Checkpoint saved: 100 queries processed

============================================================
Batch 59/198: Processing 2 queries
============================================================
Predictions:  ['2', '3 1']
Answer:  ['2', 'false']
Predictions:  ['2']
Answer:  ['2']
  [Batch] Query 2792: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.93s avg)
Predictions:  ['3 1']
Answer:  ['false']
  [Batch] Query 2793: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.93s avg)
Checkpoint saved: 102 queries processed

============================================================
Batch 60/198: Processing 2 queries
============================================================
Predictions:  ['43', '2 2']
Answer:  ['43', '2']
Predictions:  ['43']
Answer:  ['43']
  [Batch] Query 2794: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.02s avg)
Predictions:  ['2 2']
Answer:  ['2']
  [Batch] Query 2795: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.02s avg)
Checkpoint saved: 104 queries processed

============================================================
Batch 61/198: Processing 2 queries
============================================================
Predictions:  ['dom psc', '2']
Answer:  ['2', '2']
Predictions:  ['dom psc']
Answer:  ['2']
  [Batch] Query 2796: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.90s avg)
Predictions:  ['2']
Answer:  ['2']
  [Batch] Query 2797: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.90s avg)
Checkpoint saved: 106 queries processed

============================================================
Batch 62/198: Processing 2 queries
============================================================
Predictions:  ['yes', 'yes']
Answer:  ['no', 'yes']
Predictions:  ['yes']
Answer:  ['no']
  [Batch] Query 2798: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.23s avg)
Predictions:  ['yes']
Answer:  ['yes']
  [Batch] Query 2799: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.23s avg)
Checkpoint saved: 108 queries processed

============================================================
Batch 63/198: Processing 2 queries
============================================================
Predictions:  ['answering machine no answer', '£29']
Answer:  ['no answer answering machine call completed busy dial error callback operator intercept do not call abandon hangup regret', '29']
Predictions:  ['answering machine no answer']
Answer:  ['no answer answering machine call completed busy dial error callback operator intercept do not call abandon hangup regret']
  [Batch] Query 2800: {'F1': 36.36, 'EM': 0.0, 'ROUGE-L': 18.18, 'SacreBLEU': 1.36} (3.43s avg)
Predictions:  ['£29']
Answer:  ['29']
  [Batch] Query 2801: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (3.43s avg)
Checkpoint saved: 110 queries processed

============================================================
Batch 64/198: Processing 2 queries
============================================================
Predictions:  ['colwyn staplehurst', '2']
Answer:  ['colwyn', '3']
Predictions:  ['colwyn staplehurst']
Answer:  ['colwyn']
  [Batch] Query 2802: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (4.12s avg)
Predictions:  ['2']
Answer:  ['3']
  [Batch] Query 2803: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (4.12s avg)
Checkpoint saved: 112 queries processed

============================================================
Batch 65/198: Processing 2 queries
============================================================
Predictions:  ['cambourne colwyn', 'jones fonseca riblet evan horton arthur melf rogelio torres randy staples jessi cambra john segada cody eacret rogelio boyd randy']
Answer:  ['bexhill colwyn', 'arthur horton rogelio melf randy torres jessi staples john cambra randy boyd']
Predictions:  ['cambourne colwyn']
Answer:  ['bexhill colwyn']
Processing batches:  33%|███▎      | 65/198 [09:08<16:49,  7.59s/it]Processing batches:  33%|███▎      | 66/198 [13:42<3:12:52, 87.67s/it]Processing batches:  34%|███▍      | 67/198 [13:49<2:18:34, 63.47s/it]Processing batches:  34%|███▍      | 68/198 [14:04<1:46:17, 49.06s/it]  [Batch] Query 2804: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (5.72s avg)
Predictions:  ['jones fonseca riblet evan horton arthur melf rogelio torres randy staples jessi cambra john segada cody eacret rogelio boyd randy']
Answer:  ['arthur horton rogelio melf randy torres jessi staples john cambra randy boyd']
  [Batch] Query 2805: {'F1': 75.0, 'EM': 0.0, 'ROUGE-L': 37.5, 'SacreBLEU': 3.56} (5.72s avg)
Checkpoint saved: 114 queries processed

============================================================
Batch 66/198: Processing 2 queries
============================================================
Predictions:  ['15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 1', 'hewitt shane']
Answer:  ['12', 'shane hewitt']
Predictions:  ['15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 1']
Answer:  ['12']
  [Batch] Query 2806: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (137.14s avg)
Predictions:  ['hewitt shane']
Answer:  ['shane hewitt']
  [Batch] Query 2807: {'F1': 100.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (137.14s avg)
Checkpoint saved: 116 queries processed

============================================================
Batch 67/198: Processing 2 queries
============================================================
Predictions:  ['horton arthur', 'jones fonseca riblet evan horton arthur melf rogelio torres randy staples jessi cambra john segada cody eacret rogelio boyd randy']
Answer:  ['arthur horton', 'arthur horton rogelio melf randy torres jessi staples john cambra randy boyd']
Predictions:  ['horton arthur']
Answer:  ['arthur horton']
  [Batch] Query 2808: {'F1': 100.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (3.38s avg)
Predictions:  ['jones fonseca riblet evan horton arthur melf rogelio torres randy staples jessi cambra john segada cody eacret rogelio boyd randy']
Answer:  ['arthur horton rogelio melf randy torres jessi staples john cambra randy boyd']
  [Batch] Query 2809: {'F1': 75.0, 'EM': 0.0, 'ROUGE-L': 37.5, 'SacreBLEU': 3.56} (3.38s avg)
Checkpoint saved: 118 queries processed

============================================================
Batch 68/198: Processing 2 queries
============================================================
Predictions:  ['2406 2885', 'sive2024cbo31768 07112024']
Answer:  ['2406', 'sive2024cbo31768']
Predictions:  ['2406 2885']
Answer:  ['2406']
  [Batch] Query 2810: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (7.59s avg)
Predictions:  ['sive2024cbo31768 07112024']
Answer:  ['sive2024cbo31768']
  [Batch] Query 2811: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (7.59s avg)
Checkpoint saved: 120 queries processed

============================================================
Batch 69/198: Processing 2 queries
============================================================
Predictions:  ['lower branch i centeral branch o ground branch h inter branch', 'dragon pvt ltd g co pvt']
Answer:  ['4 centeral branch o lower branch i ground branch h inter branch', 'g co pvt']
Predictions:  ['lower branch i centeral branch o ground branch h inter branch']
Answer:  ['4 centeral branch o lower branch i ground branch h inter branch']
  [Batch] Query 2812: {'F1': 95.65, 'EM': 0.0, 'ROUGE-L': 69.57, 'SacreBLEU': 52.72} (6.98s avg)
Predictions:  ['dragon pvt ltd g co pvt']
Answer:  ['g co pvt']
Processing batches:  35%|███▍      | 69/198 [14:19<1:22:58, 38.60s/it]Processing batches:  35%|███▌      | 70/198 [14:22<59:36, 27.94s/it]  Processing batches:  36%|███▌      | 71/198 [14:25<43:18, 20.46s/it]Processing batches:  36%|███▋      | 72/198 [14:39<39:00, 18.57s/it]Processing batches:  37%|███▋      | 73/198 [14:53<36:06, 17.33s/it]Processing batches:  37%|███▋      | 74/198 [14:56<26:58, 13.05s/it]Processing batches:  38%|███▊      | 75/198 [15:00<20:47, 10.14s/it]Processing batches:  38%|███▊      | 76/198 [15:06<18:22,  9.04s/it]Processing batches:  39%|███▉      | 77/198 [15:13<16:54,  8.39s/it]Processing batches:  39%|███▉      | 78/198 [15:22<17:22,  8.68s/it]Processing batches:  40%|███▉      | 79/198 [15:31<17:00,  8.58s/it]Processing batches:  40%|████      | 80/198 [15:33<13:19,  6.77s/it]  [Batch] Query 2813: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 30.21} (6.98s avg)
Checkpoint saved: 122 queries processed

============================================================
Batch 70/198: Processing 2 queries
============================================================
Predictions:  ['13557', 'mobile department other department']
Answer:  ['13557', 'mobile department']
Predictions:  ['13557']
Answer:  ['13557']
  [Batch] Query 2814: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.43s avg)
Predictions:  ['mobile department other department']
Answer:  ['mobile department']
  [Batch] Query 2815: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 31.95} (1.43s avg)
Checkpoint saved: 124 queries processed

============================================================
Batch 71/198: Processing 2 queries
============================================================
Predictions:  ['workstation register', '94']
Answer:  ['2', '94']
Predictions:  ['workstation register']
Answer:  ['2']
  [Batch] Query 2816: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.39s avg)
Predictions:  ['94']
Answer:  ['94']
  [Batch] Query 2817: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.39s avg)
Checkpoint saved: 126 queries processed

============================================================
Batch 72/198: Processing 2 queries
============================================================
Predictions:  ['eur 10 aud 105', 'mg']
Answer:  ['australia', 'mg']
Predictions:  ['eur 10 aud 105']
Answer:  ['australia']
  [Batch] Query 2818: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (6.97s avg)
Predictions:  ['mg']
Answer:  ['mg']
  [Batch] Query 2819: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (6.97s avg)
Checkpoint saved: 128 queries processed

============================================================
Batch 73/198: Processing 2 queries
============================================================
Predictions:  ['3month premium subscription 1month premium subscription', 'canada japan']
Answer:  ['1month premium subscription', 'canada']
Predictions:  ['3month premium subscription 1month premium subscription']
Answer:  ['1month premium subscription']
  [Batch] Query 2820: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 30.21} (7.09s avg)
Predictions:  ['canada japan']
Answer:  ['canada']
  [Batch] Query 2821: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (7.09s avg)
Checkpoint saved: 130 queries processed

============================================================
Batch 74/198: Processing 2 queries
============================================================
Predictions:  ['new activation add line plan change upgrade', '7369']
Answer:  ['new activation add line plan change upgrade', '7369']
Predictions:  ['new activation add line plan change upgrade']
Answer:  ['new activation add line plan change upgrade']
  [Batch] Query 2822: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (1.42s avg)
Predictions:  ['7369']
Answer:  ['7369']
  [Batch] Query 2823: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.42s avg)
Checkpoint saved: 132 queries processed

============================================================
Batch 75/198: Processing 2 queries
============================================================
Predictions:  ['vzw 183', '142 146 142']
Answer:  ['spr 206', '143']
Predictions:  ['vzw 183']
Answer:  ['spr 206']
  [Batch] Query 2824: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.55s avg)
Predictions:  ['142 146 142']
Answer:  ['143']
  [Batch] Query 2825: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.55s avg)
Checkpoint saved: 134 queries processed

============================================================
Batch 76/198: Processing 2 queries
============================================================
Predictions:  ['26227642', 'giorgio armani francois chevalier mia martina tia carrera feliciano di venturi global bob vincenzo fertino bob bobitt ken cascaval billy joe giuseppe von firenze']
Answer:  ['26227642', 'francois chevalier 8607203 global bob 10388812']
Predictions:  ['26227642']
Answer:  ['26227642']
  [Batch] Query 2826: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (3.11s avg)
Predictions:  ['giorgio armani francois chevalier mia martina tia carrera feliciano di venturi global bob vincenzo fertino bob bobitt ken cascaval billy joe giuseppe von firenze']
Answer:  ['francois chevalier 8607203 global bob 10388812']
  [Batch] Query 2827: {'F1': 26.67, 'EM': 0.0, 'ROUGE-L': 26.67, 'SacreBLEU': 4.45} (3.11s avg)
Checkpoint saved: 136 queries processed

============================================================
Batch 77/198: Processing 2 queries
============================================================
Predictions:  ['big joe maria calux giorgio armani francois chevalier mia martina tia carrera feliciano di venturi global bob vincenzo fertino bob bobitt ken cascaval billy joe giuseppe von firenze', 'big joe 1728302']
Answer:  ['3', 'big joe 18728302']
Predictions:  ['big joe maria calux giorgio armani francois chevalier mia martina tia carrera feliciano di venturi global bob vincenzo fertino bob bobitt ken cascaval billy joe giuseppe von firenze']
Answer:  ['3']
  [Batch] Query 2828: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.32s avg)
Predictions:  ['big joe 1728302']
Answer:  ['big joe 18728302']
  [Batch] Query 2829: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (3.32s avg)
Checkpoint saved: 138 queries processed

============================================================
Batch 78/198: Processing 2 queries
============================================================
Predictions:  ['argentinos jrs talleres cordoba san martin sj independiente boca juniors', 'fc slutsk shakhter soligorsk']
Answer:  ['argentinos jrs', 'fc slutsk shakhter soligorsk dinamo minsk bate borisov']
Predictions:  ['argentinos jrs talleres cordoba san martin sj independiente boca juniors']
Answer:  ['argentinos jrs']
  [Batch] Query 2830: {'F1': 33.33, 'EM': 0.0, 'ROUGE-L': 33.33, 'SacreBLEU': 8.39} (4.57s avg)
Predictions:  ['fc slutsk shakhter soligorsk']
Answer:  ['fc slutsk shakhter soligorsk dinamo minsk bate borisov']
  [Batch] Query 2831: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 36.79} (4.57s avg)
Checkpoint saved: 140 queries processed

============================================================
Batch 79/198: Processing 2 queries
============================================================
Predictions:  ['brazilserie 13', 'argentinasuperliga argentina austriabundesliga']
Answer:  ['argentinasuperliga argentina 23', 'equal 2 matches each']
Predictions:  ['brazilserie 13']
Answer:  ['argentinasuperliga argentina 23']
  [Batch] Query 2832: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (4.05s avg)
Predictions:  ['argentinasuperliga argentina austriabundesliga']
Answer:  ['equal 2 matches each']
  [Batch] Query 2833: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (4.05s avg)
Checkpoint saved: 142 queries processed

============================================================
Batch 80/198: Processing 2 queries
============================================================
Predictions:  ['130700', 'b2 732288']
Answer:  ['150667', 'b2 732288']
Predictions:  ['130700']
Answer:  ['150667']
  [Batch] Query 2834: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.16s avg)
Predictions:  ['b2 732288']
Answer:  ['b2 732288']
  [Batch] Query 2835: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.16s avg)
Checkpoint saved: 144 queries processed

============================================================
Batch 81/198: Processing 2 queries
============================================================
Predictions:  ['5 bob', '3650 3650']
Answer:  ['5 bob', '3833']
Predictions:  ['5 bob']
Answer:  ['5 bob']
  [Batch] Query 2836: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.32s avg)
Predictions:  ['3650 3650']
Answer:  ['3833']
Processing batches:  41%|████      | 81/198 [15:36<10:55,  5.60s/it]Processing batches:  41%|████▏     | 82/198 [15:39<09:14,  4.78s/it]Processing batches:  42%|████▏     | 83/198 [15:42<08:10,  4.27s/it]Processing batches:  42%|████▏     | 84/198 [15:44<06:58,  3.67s/it]Processing batches:  43%|████▎     | 85/198 [15:47<06:03,  3.21s/it]Processing batches:  43%|████▎     | 86/198 [15:49<05:50,  3.13s/it]Processing batches:  44%|████▍     | 87/198 [15:53<05:44,  3.10s/it]Processing batches:  44%|████▍     | 88/198 [15:56<05:38,  3.08s/it]Processing batches:  45%|████▍     | 89/198 [15:59<05:58,  3.29s/it]Processing batches:  45%|████▌     | 90/198 [16:05<06:59,  3.89s/it]Processing batches:  46%|████▌     | 91/198 [16:20<12:48,  7.18s/it]Processing batches:  46%|████▋     | 92/198 [16:24<11:06,  6.29s/it]Processing batches:  47%|████▋     | 93/198 [16:28<10:10,  5.81s/it]  [Batch] Query 2837: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.32s avg)
Checkpoint saved: 146 queries processed

============================================================
Batch 82/198: Processing 2 queries
============================================================
Predictions:  ['b', 'yes']
Answer:  ['e', 'yes']
Predictions:  ['b']
Answer:  ['e']
  [Batch] Query 2838: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.31s avg)
Predictions:  ['yes']
Answer:  ['yes']
  [Batch] Query 2839: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.31s avg)
Checkpoint saved: 148 queries processed

============================================================
Batch 83/198: Processing 2 queries
============================================================
Predictions:  ['#', 'product f product']
Answer:  ['', 'f']
Predictions:  ['#']
Answer:  ['']
  [Batch] Query 2840: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.42s avg)
Predictions:  ['product f product']
Answer:  ['f']
  [Batch] Query 2841: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (1.42s avg)
Checkpoint saved: 150 queries processed

============================================================
Batch 84/198: Processing 2 queries
============================================================
Predictions:  ['west central north east', 'football 14']
Answer:  ['yes', 'football 14']
Predictions:  ['west central north east']
Answer:  ['yes']
  [Batch] Query 2842: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.02s avg)
Predictions:  ['football 14']
Answer:  ['football 14']
  [Batch] Query 2843: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.02s avg)
Checkpoint saved: 152 queries processed

============================================================
Batch 85/198: Processing 2 queries
============================================================
Predictions:  ['1 1', 'north']
Answer:  ['2', 'true']
Predictions:  ['1 1']
Answer:  ['2']
  [Batch] Query 2844: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (0.95s avg)
Predictions:  ['north']
Answer:  ['true']
  [Batch] Query 2845: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (0.95s avg)
Checkpoint saved: 154 queries processed

============================================================
Batch 86/198: Processing 2 queries
============================================================
Predictions:  ['region commission override', 'salary territory and team personal led mbo']
Answer:  ['region commission override', 'salary territory and team personal led mbo']
Predictions:  ['region commission override']
Answer:  ['region commission override']
  [Batch] Query 2846: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.34s avg)
Predictions:  ['salary territory and team personal led mbo']
Answer:  ['salary territory and team personal led mbo']
  [Batch] Query 2847: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (1.34s avg)
Checkpoint saved: 156 queries processed

============================================================
Batch 87/198: Processing 2 queries
============================================================
Predictions:  ['territory and team personal led salary mbo', 'salary personal commission new rep quota mbos']
Answer:  ['territory and team personal led salary mbo', '2']
Predictions:  ['territory and team personal led salary mbo']
Answer:  ['territory and team personal led salary mbo']
  [Batch] Query 2848: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (1.41s avg)
Predictions:  ['salary personal commission new rep quota mbos']
Answer:  ['2']
  [Batch] Query 2849: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.41s avg)
Checkpoint saved: 158 queries processed

============================================================
Batch 88/198: Processing 2 queries
============================================================
Predictions:  ['navel mandrin red', 'no']
Answer:  ['navel mandrin red', 'no']
Predictions:  ['navel mandrin red']
Answer:  ['navel mandrin red']
  [Batch] Query 2850: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.39s avg)
Predictions:  ['no']
Answer:  ['no']
  [Batch] Query 2851: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.39s avg)
Checkpoint saved: 160 queries processed

============================================================
Batch 89/198: Processing 2 queries
============================================================
Predictions:  ['81 73 45 44 41', 'navel mandrin red']
Answer:  ['284', 'navel red mandrin']
Predictions:  ['81 73 45 44 41']
Answer:  ['284']
  [Batch] Query 2852: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.77s avg)
Predictions:  ['navel mandrin red']
Answer:  ['navel red mandrin']
  [Batch] Query 2853: {'F1': 100.0, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (1.77s avg)
Checkpoint saved: 162 queries processed

============================================================
Batch 90/198: Processing 2 queries
============================================================
Predictions:  ['europe eur japan jpy', 'new zealand nzd']
Answer:  ['europe eur', 'europe ils middle east sar middle east aed']
Predictions:  ['europe eur japan jpy']
Answer:  ['europe eur']
  [Batch] Query 2854: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 31.95} (2.52s avg)
Predictions:  ['new zealand nzd']
Answer:  ['europe ils middle east sar middle east aed']
  [Batch] Query 2855: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.52s avg)
Checkpoint saved: 164 queries processed

============================================================
Batch 91/198: Processing 2 queries
============================================================
Predictions:  ['canada north america europe eur europe gbp europe chf europe dkk europe sek europe nok europe ils middle east sar middle east aed japan asia pacific twd asia pacific krw asia pacific hkd australia new zealand philippines', '129 129 95 125 71 87 755 47 50 50 1500 380 13700 100 150 105 19 46 65 13 19 46 75 250 500 650 1550 250']
Answer:  ['12', '115']
Predictions:  ['canada north america europe eur europe gbp europe chf europe dkk europe sek europe nok europe ils middle east sar middle east aed japan asia pacific twd asia pacific krw asia pacific hkd australia new zealand philippines']
Answer:  ['12']
  [Batch] Query 2856: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (7.32s avg)
Predictions:  ['129 129 95 125 71 87 755 47 50 50 1500 380 13700 100 150 105 19 46 65 13 19 46 75 250 500 650 1550 250']
Answer:  ['115']
  [Batch] Query 2857: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (7.32s avg)
Checkpoint saved: 166 queries processed

============================================================
Batch 92/198: Processing 2 queries
============================================================
Predictions:  ['224', '179']
Answer:  ['224', '179']
Predictions:  ['224']
Answer:  ['224']
  [Batch] Query 2858: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.99s avg)
Predictions:  ['179']
Answer:  ['179']
  [Batch] Query 2859: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.99s avg)
Checkpoint saved: 168 queries processed

============================================================
Batch 93/198: Processing 2 queries
============================================================
Predictions:  ['daikon notops', '117 152']
Answer:  ['notops', '269']
Predictions:  ['daikon notops']
Answer:  ['notops']
  [Batch] Query 2860: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.23s avg)
Predictions:  ['117 152']
Answer:  ['269']
  [Batch] Query 2861: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.23s avg)
Checkpoint saved: 170 queries processed

============================================================
Batch 94/198: Processing 2 queries
============================================================
Predictions:  ['23', '23 30']
Answer:  ['23', '20year historical average']
Predictions:  ['23']
Answer:  ['23']
Processing batches:  47%|████▋     | 94/198 [16:34<09:47,  5.65s/it]Processing batches:  48%|████▊     | 95/198 [16:39<09:22,  5.47s/it]Processing batches:  48%|████▊     | 96/198 [16:43<08:53,  5.23s/it]Processing batches:  49%|████▉     | 97/198 [16:48<08:33,  5.09s/it]Processing batches:  49%|████▉     | 98/198 [16:56<09:58,  5.99s/it]Processing batches:  50%|█████     | 99/198 [17:07<12:14,  7.42s/it]Processing batches:  51%|█████     | 100/198 [17:11<10:13,  6.26s/it]Processing batches:  51%|█████     | 101/198 [17:15<09:22,  5.80s/it]Processing batches:  52%|█████▏    | 102/198 [17:29<12:59,  8.12s/it]Processing batches:  52%|█████▏    | 103/198 [17:43<15:52, 10.03s/it]Processing batches:  53%|█████▎    | 104/198 [17:55<16:37, 10.61s/it]Processing batches:  53%|█████▎    | 105/198 [18:09<17:57, 11.59s/it]Processing batches:  54%|█████▎    | 106/198 [18:49<30:48, 20.09s/it]  [Batch] Query 2862: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.52s avg)
Predictions:  ['23 30']
Answer:  ['20year historical average']
  [Batch] Query 2863: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.52s avg)
Checkpoint saved: 172 queries processed

============================================================
Batch 95/198: Processing 2 queries
============================================================
Predictions:  ['21 23 26', '1']
Answer:  ['21 23 26', '1']
Predictions:  ['21 23 26']
Answer:  ['21 23 26']
  [Batch] Query 2864: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.40s avg)
Predictions:  ['1']
Answer:  ['1']
  [Batch] Query 2865: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.40s avg)
Checkpoint saved: 174 queries processed

============================================================
Batch 96/198: Processing 2 queries
============================================================
Predictions:  ['206135 58963', '2015']
Answer:  ['206135', '2015']
Predictions:  ['206135 58963']
Answer:  ['206135']
  [Batch] Query 2866: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.22s avg)
Predictions:  ['2015']
Answer:  ['2015']
  [Batch] Query 2867: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.22s avg)
Checkpoint saved: 176 queries processed

============================================================
Batch 97/198: Processing 2 queries
============================================================
Predictions:  ['2015 2014', '2015q3 2015q4']
Answer:  ['2015', '2']
Predictions:  ['2015 2014']
Answer:  ['2015']
  [Batch] Query 2868: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.25s avg)
Predictions:  ['2015q3 2015q4']
Answer:  ['2']
  [Batch] Query 2869: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.25s avg)
Checkpoint saved: 178 queries processed

============================================================
Batch 98/198: Processing 2 queries
============================================================
Predictions:  ['2884', 'q4']
Answer:  ['3026', 'q4']
Predictions:  ['2884']
Answer:  ['3026']
  [Batch] Query 2870: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.93s avg)
Predictions:  ['q4']
Answer:  ['q4']
  [Batch] Query 2871: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (3.93s avg)
Checkpoint saved: 180 queries processed

============================================================
Batch 99/198: Processing 2 queries
============================================================
Predictions:  ['217', 'unduplicated total ed default management system ed federal loan servicers guaranty agency held ed total and permanent disability servicer commercial lender held']
Answer:  ['220', 'unduplicated total commercial lender held ed federal loan servicers guaranty agency held ed default management system ed total and permanent disability servicer']
Predictions:  ['217']
Answer:  ['220']
  [Batch] Query 2872: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (5.25s avg)
Predictions:  ['unduplicated total ed default management system ed federal loan servicers guaranty agency held ed total and permanent disability servicer commercial lender held']
Answer:  ['unduplicated total commercial lender held ed federal loan servicers guaranty agency held ed default management system ed total and permanent disability servicer']
  [Batch] Query 2873: {'F1': 100.0, 'EM': 0.0, 'ROUGE-L': 68.18, 'SacreBLEU': 75.98} (5.25s avg)
Checkpoint saved: 182 queries processed

============================================================
Batch 100/198: Processing 2 queries
============================================================
Predictions:  ['1675', 'fy2012 fy2009']
Answer:  ['1675', 'fy2012']
Predictions:  ['1675']
Answer:  ['1675']
  [Batch] Query 2874: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.66s avg)
Predictions:  ['fy2012 fy2009']
Answer:  ['fy2012']
  [Batch] Query 2875: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (1.66s avg)
Checkpoint saved: 184 queries processed

============================================================
Batch 101/198: Processing 2 queries
============================================================
Predictions:  ['fy2011 fy2012 fy2010 fy2009', '47']
Answer:  ['fy2011 fy2012 fy2010 fy2009', '47']
Predictions:  ['fy2011 fy2012 fy2010 fy2009']
Answer:  ['fy2011 fy2012 fy2010 fy2009']
  [Batch] Query 2876: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (2.25s avg)
Predictions:  ['47']
Answer:  ['47']
  [Batch] Query 2877: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.25s avg)
Checkpoint saved: 186 queries processed

============================================================
Batch 102/198: Processing 2 queries
============================================================
Predictions:  ['2679763', 'applied studies human medicine or dentistry applied studies veterinary medicine']
Answer:  ['2679763', 'human medicine or dentistry']
Predictions:  ['2679763']
Answer:  ['2679763']
  [Batch] Query 2878: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (6.65s avg)
Predictions:  ['applied studies human medicine or dentistry applied studies veterinary medicine']
Answer:  ['human medicine or dentistry']
  [Batch] Query 2879: {'F1': 57.14, 'EM': 0.0, 'ROUGE-L': 57.14, 'SacreBLEU': 26.27} (6.65s avg)
Checkpoint saved: 188 queries processed

============================================================
Batch 103/198: Processing 2 queries
============================================================
Predictions:  ['318304 137751 7811', 'mouse rat guinea pig']
Answer:  ['463866', 'mouse rat guinea pig']
Predictions:  ['318304 137751 7811']
Answer:  ['463866']
  [Batch] Query 2880: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (7.12s avg)
Predictions:  ['mouse rat guinea pig']
Answer:  ['mouse rat guinea pig']
  [Batch] Query 2881: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (7.12s avg)
Checkpoint saved: 190 queries processed

============================================================
Batch 104/198: Processing 2 queries
============================================================
Predictions:  ['yes', 'mouse']
Answer:  ['yes', 'mouse']
Predictions:  ['yes']
Answer:  ['yes']
  [Batch] Query 2882: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (5.87s avg)
Predictions:  ['mouse']
Answer:  ['mouse']
  [Batch] Query 2883: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (5.87s avg)
Checkpoint saved: 192 queries processed

============================================================
Batch 105/198: Processing 2 queries
============================================================
Predictions:  ['mouse rat', '828857 255627']
Answer:  ['mouse fish', 'mouse']
Predictions:  ['mouse rat']
Answer:  ['mouse fish']
  [Batch] Query 2884: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (6.81s avg)
Predictions:  ['828857 255627']
Answer:  ['mouse']
  [Batch] Query 2885: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (6.81s avg)
Checkpoint saved: 194 queries processed

============================================================
Batch 106/198: Processing 2 queries
============================================================
Predictions:  ['mouse', 'normal animal']
Answer:  ['mouse', 'normal animal']
Predictions:  ['mouse']
Answer:  ['mouse']
  [Batch] Query 2886: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (19.84s avg)
Predictions:  ['normal animal']
Answer:  ['normal animal']
  [Batch] Query 2887: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (19.84s avg)
Checkpoint saved: 196 queries processed

============================================================
Batch 107/198: Processing 2 queries
============================================================
Predictions:  ['1763474', 'fundamental biological research']
Answer:  ['1763474', 'breeding of gm or hm']
Predictions:  ['1763474']
Answer:  ['1763474']
Processing batches:  54%|█████▍    | 107/198 [19:31<40:31, 26.72s/it]Processing batches:  55%|█████▍    | 108/198 [19:33<29:02, 19.37s/it]Processing batches:  55%|█████▌    | 109/198 [19:36<21:06, 14.23s/it]Processing batches:  56%|█████▌    | 110/198 [19:44<18:17, 12.48s/it]Processing batches:  56%|█████▌    | 111/198 [19:52<16:06, 11.11s/it]Processing batches:  57%|█████▋    | 112/198 [19:55<12:26,  8.68s/it]Processing batches:  57%|█████▋    | 113/198 [19:58<09:45,  6.89s/it]Processing batches:  58%|█████▊    | 114/198 [20:00<07:53,  5.64s/it]Processing batches:  58%|█████▊    | 115/198 [20:03<06:42,  4.84s/it]Processing batches:  59%|█████▊    | 116/198 [20:06<05:33,  4.06s/it]Processing batches:  59%|█████▉    | 117/198 [20:08<04:46,  3.53s/it]Processing batches:  60%|█████▉    | 118/198 [20:11<04:28,  3.36s/it]Processing batches:  60%|██████    | 119/198 [20:13<03:59,  3.03s/it]Processing batches:  61%|██████    | 120/198 [20:16<03:57,  3.04s/it]Processing batches:  61%|██████    | 121/198 [20:20<04:00,  3.13s/it]  [Batch] Query 2888: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (20.98s avg)
Predictions:  ['fundamental biological research']
Answer:  ['breeding of gm or hm']
  [Batch] Query 2889: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (20.98s avg)
Checkpoint saved: 198 queries processed

============================================================
Batch 108/198: Processing 2 queries
============================================================
Predictions:  ['164', '1135']
Answer:  ['164', '1127']
Predictions:  ['164']
Answer:  ['164']
  [Batch] Query 2890: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (0.98s avg)
Predictions:  ['1135']
Answer:  ['1127']
  [Batch] Query 2891: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (0.98s avg)
Checkpoint saved: 200 queries processed

============================================================
Batch 109/198: Processing 2 queries
============================================================
Predictions:  ['4', '240 21']
Answer:  ['8', '143']
Predictions:  ['4']
Answer:  ['8']
  [Batch] Query 2892: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.00s avg)
Predictions:  ['240 21']
Answer:  ['143']
  [Batch] Query 2893: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.00s avg)
Checkpoint saved: 202 queries processed

============================================================
Batch 110/198: Processing 2 queries
============================================================
Predictions:  ['40098 67020', 'hong kong sar israel japan republic of korea singapore kazakhstan kyrgyzstan']
Answer:  ['107118', '1']
Predictions:  ['40098 67020']
Answer:  ['107118']
  [Batch] Query 2894: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (4.07s avg)
Predictions:  ['hong kong sar israel japan republic of korea singapore kazakhstan kyrgyzstan']
Answer:  ['1']
  [Batch] Query 2895: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (4.07s avg)
Checkpoint saved: 204 queries processed

============================================================
Batch 111/198: Processing 2 queries
============================================================
Predictions:  ['japan republic of korea singapore kazakhstan kyrgyzstan', 'denmark']
Answer:  ['5', 'norway']
Predictions:  ['japan republic of korea singapore kazakhstan kyrgyzstan']
Answer:  ['5']
  [Batch] Query 2896: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.84s avg)
Predictions:  ['denmark']
Answer:  ['norway']
  [Batch] Query 2897: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.84s avg)
Checkpoint saved: 206 queries processed

============================================================
Batch 112/198: Processing 2 queries
============================================================
Predictions:  ['02', '02']
Answer:  ['02', '02']
Predictions:  ['02']
Answer:  ['02']
  [Batch] Query 2898: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.39s avg)
Predictions:  ['02']
Answer:  ['02']
  [Batch] Query 2899: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.39s avg)
Checkpoint saved: 208 queries processed

============================================================
Batch 113/198: Processing 2 queries
============================================================
Predictions:  ['120', '02']
Answer:  ['day 64', '02']
Predictions:  ['120']
Answer:  ['day 64']
  [Batch] Query 2900: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.23s avg)
Predictions:  ['02']
Answer:  ['02']
  [Batch] Query 2901: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.23s avg)
Checkpoint saved: 210 queries processed

============================================================
Batch 114/198: Processing 2 queries
============================================================
Predictions:  ['07', '00']
Answer:  ['07', '00']
Predictions:  ['07']
Answer:  ['07']
  [Batch] Query 2902: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.24s avg)
Predictions:  ['00']
Answer:  ['00']
  [Batch] Query 2903: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.24s avg)
Checkpoint saved: 212 queries processed

============================================================
Batch 115/198: Processing 2 queries
============================================================
Predictions:  ['537', '5']
Answer:  ['537', '4']
Predictions:  ['537']
Answer:  ['537']
  [Batch] Query 2904: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.38s avg)
Predictions:  ['5']
Answer:  ['4']
  [Batch] Query 2905: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.38s avg)
Checkpoint saved: 214 queries processed

============================================================
Batch 116/198: Processing 2 queries
============================================================
Predictions:  ['11630', '746 829']
Answer:  ['13392', 'increase']
Predictions:  ['11630']
Answer:  ['13392']
  [Batch] Query 2906: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.00s avg)
Predictions:  ['746 829']
Answer:  ['increase']
  [Batch] Query 2907: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.00s avg)
Checkpoint saved: 216 queries processed

============================================================
Batch 117/198: Processing 2 queries
============================================================
Predictions:  ['1267 333', 'mouse 3']
Answer:  ['759', 'mouse 3']
Predictions:  ['1267 333']
Answer:  ['759']
  [Batch] Query 2908: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.03s avg)
Predictions:  ['mouse 3']
Answer:  ['mouse 3']
  [Batch] Query 2909: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.03s avg)
Checkpoint saved: 218 queries processed

============================================================
Batch 118/198: Processing 2 queries
============================================================
Predictions:  ['1485', '12745 12205']
Answer:  ['1948', 'increase']
Predictions:  ['1485']
Answer:  ['1948']
  [Batch] Query 2910: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.35s avg)
Predictions:  ['12745 12205']
Answer:  ['increase']
  [Batch] Query 2911: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.35s avg)
Checkpoint saved: 220 queries processed

============================================================
Batch 119/198: Processing 2 queries
============================================================
Predictions:  ['399', 'mouse 2']
Answer:  ['36', 'mouse 2']
Predictions:  ['399']
Answer:  ['36']
  [Batch] Query 2912: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.02s avg)
Predictions:  ['mouse 2']
Answer:  ['mouse 2']
  [Batch] Query 2913: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.02s avg)
Checkpoint saved: 222 queries processed

============================================================
Batch 120/198: Processing 2 queries
============================================================
Predictions:  ['7643', 'gland 2']
Answer:  ['8364', 'gland 4']
Predictions:  ['7643']
Answer:  ['8364']
  [Batch] Query 2914: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.42s avg)
Predictions:  ['gland 2']
Answer:  ['gland 4']
  [Batch] Query 2915: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (1.42s avg)
Checkpoint saved: 224 queries processed

============================================================
Batch 121/198: Processing 2 queries
============================================================
Predictions:  ['lower', '00 00']
Answer:  ['lower', '00']
Predictions:  ['lower']
Answer:  ['lower']
  [Batch] Query 2916: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.54s avg)
Predictions:  ['00 00']
Answer:  ['00']
  [Batch] Query 2917: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (1.54s avg)
Checkpoint saved: 226 queries processed

============================================================
Batch 122/198: Processing 2 queries
============================================================
Predictions:  ['756 303', 'yes']
Answer:  Processing batches:  62%|██████▏   | 122/198 [20:22<03:52,  3.05s/it]Processing batches:  62%|██████▏   | 123/198 [20:25<03:43,  2.98s/it]Processing batches:  63%|██████▎   | 124/198 [20:28<03:40,  2.98s/it]Processing batches:  63%|██████▎   | 125/198 [20:32<03:44,  3.07s/it]Processing batches:  64%|██████▎   | 126/198 [20:35<03:45,  3.13s/it]Processing batches:  64%|██████▍   | 127/198 [20:38<03:45,  3.17s/it]Processing batches:  65%|██████▍   | 128/198 [20:43<04:19,  3.71s/it]['756 of sample is in 60 to 64 years age group with 303 achieving optimal condition', 'yes']
Predictions:  ['756 303']
Answer:  ['756 of sample is in 60 to 64 years age group with 303 achieving optimal condition']
  [Batch] Query 2918: {'F1': 22.22, 'EM': 0.0, 'ROUGE-L': 22.22, 'SacreBLEU': 0.0} (1.32s avg)
Predictions:  ['yes']
Answer:  ['yes']
  [Batch] Query 2919: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.32s avg)
Checkpoint saved: 228 queries processed

============================================================
Batch 123/198: Processing 2 queries
============================================================
Predictions:  ['no yes', '031096']
Answer:  ['yes', '0310']
Predictions:  ['no yes']
Answer:  ['yes']
  [Batch] Query 2920: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (1.29s avg)
Predictions:  ['031096']
Answer:  ['0310']
  [Batch] Query 2921: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.29s avg)
Checkpoint saved: 230 queries processed

============================================================
Batch 124/198: Processing 2 queries
============================================================
Predictions:  ['no', '171']
Answer:  ['no', '171']
Predictions:  ['no']
Answer:  ['no']
  [Batch] Query 2922: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.36s avg)
Predictions:  ['171']
Answer:  ['171']
  [Batch] Query 2923: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.36s avg)
Checkpoint saved: 232 queries processed

============================================================
Batch 125/198: Processing 2 queries
============================================================
Predictions:  ['78', '012067']
Answer:  ['78', '0107']
Predictions:  ['78']
Answer:  ['78']
  [Batch] Query 2924: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.52s avg)
Predictions:  ['012067']
Answer:  ['0107']
  [Batch] Query 2925: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.52s avg)
Checkpoint saved: 234 queries processed

============================================================
Batch 126/198: Processing 2 queries
============================================================
Predictions:  ['44', 'expt2']
Answer:  ['44', 'expt2']
Predictions:  ['44']
Answer:  ['44']
  [Batch] Query 2926: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.52s avg)
Predictions:  ['expt2']
Answer:  ['expt2']
  [Batch] Query 2927: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.52s avg)
Checkpoint saved: 236 queries processed

============================================================
Batch 127/198: Processing 2 queries
============================================================
Predictions:  ['05', '03']
Answer:  ['05', '03']
Predictions:  ['05']
Answer:  ['05']
  [Batch] Query 2928: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.52s avg)
Predictions:  ['03']
Answer:  ['03']
  [Batch] Query 2929: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.52s avg)
Checkpoint saved: 238 queries processed

============================================================
Batch 128/198: Processing 2 queries
============================================================
Predictions:  ['cryptosporidium', 'cryptosporidium spp untreated onsite wastewater cryptosporidium spp graywater']
Answer:  ['cryptosporidium', 'cryptosporidium sppuntreated onsite wastewater cryptosporidium sppgraywater']
Predictions:  ['cryptosporidium']
Answer:  ['cryptosporidium']
  [Batch] Query 2930: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.35s avg)
Predictions:  ['cryptosporidium spp untreated onsite wastewater cryptosporidium spp graywater']
Answer:  ['cryptosporidium sppuntreated onsite wastewater cryptosporidium sppgraywater']
  [Batch] Query 2931: {'F1': 57.14, 'EM': 0.0, 'ROUGE-L': 57.14, 'SacreBLEU': 22.09} (2.35s avg)
Checkpoint saved: 240 queries processed

============================================================
Batch 129/198: Processing 2 queries
============================================================
Predictions:  ['997 113 698 273 232 100 100 100 100 100 997 113 698 273 232 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 1', 'norovirus 55']
Answer:  ['766', 'norovirus 35']
Predictions:  Processing batches:  65%|██████▌   | 129/198 [25:37<1:44:18, 90.71s/it]Processing batches:  66%|██████▌   | 130/198 [25:42<1:13:45, 65.09s/it]Processing batches:  66%|██████▌   | 131/198 [25:47<52:27, 46.98s/it]  Processing batches:  67%|██████▋   | 132/198 [25:51<37:35, 34.17s/it]Processing batches:  67%|██████▋   | 133/198 [25:55<27:01, 24.94s/it]Processing batches:  68%|██████▊   | 134/198 [25:58<19:41, 18.45s/it]Processing batches:  68%|██████▊   | 135/198 [26:01<14:35, 13.90s/it]Processing batches:  69%|██████▊   | 136/198 [26:04<11:02, 10.68s/it]['997 113 698 273 232 100 100 100 100 100 997 113 698 273 232 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 1']
Answer:  ['766']
  [Batch] Query 2932: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (146.73s avg)
Predictions:  ['norovirus 55']
Answer:  ['norovirus 35']
  [Batch] Query 2933: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (146.73s avg)
Checkpoint saved: 242 queries processed

============================================================
Batch 130/198: Processing 2 queries
============================================================
Predictions:  ['roof runoff 55', 'roof runoff graywater stormwater 10 wastewater untreated onsite wastewater untreated municipal wastewater']
Answer:  ['roof runoff 55', 'roof runoff graywater stormwater 10 wastewater untreated municipal wastewater untreated onsite wastewater']
Predictions:  ['roof runoff 55']
Answer:  ['roof runoff 55']
  [Batch] Query 2934: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.53s avg)
Predictions:  ['roof runoff graywater stormwater 10 wastewater untreated onsite wastewater untreated municipal wastewater']
Answer:  ['roof runoff graywater stormwater 10 wastewater untreated municipal wastewater untreated onsite wastewater']
  [Batch] Query 2935: {'F1': 100.0, 'EM': 0.0, 'ROUGE-L': 83.33, 'SacreBLEU': 88.01} (2.53s avg)
Checkpoint saved: 244 queries processed

============================================================
Batch 131/198: Processing 2 queries
============================================================
Predictions:  ['abs plastic day 0', 'day 0']
Answer:  ['glass day 0', 'day 0']
Predictions:  ['abs plastic day 0']
Answer:  ['glass day 0']
  [Batch] Query 2936: {'F1': 57.14, 'EM': 0.0, 'ROUGE-L': 57.14, 'SacreBLEU': 31.95} (2.25s avg)
Predictions:  ['day 0']
Answer:  ['day 0']
  [Batch] Query 2937: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.25s avg)
Checkpoint saved: 246 queries processed

============================================================
Batch 132/198: Processing 2 queries
============================================================
Predictions:  ['35', 'escherichia coli']
Answer:  ['34', 'escherichia coli']
Predictions:  ['35']
Answer:  ['34']
  [Batch] Query 2938: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.02s avg)
Predictions:  ['escherichia coli']
Answer:  ['escherichia coli']
  [Batch] Query 2939: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.02s avg)
Checkpoint saved: 248 queries processed

============================================================
Batch 133/198: Processing 2 queries
============================================================
Predictions:  ['28', 'stainless steel glass abs plastic']
Answer:  ['32', 'stainless steel abs plastic glass']
Predictions:  ['28']
Answer:  ['32']
  [Batch] Query 2940: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.59s avg)
Predictions:  ['stainless steel glass abs plastic']
Answer:  ['stainless steel abs plastic glass']
  [Batch] Query 2941: {'F1': 100.0, 'EM': 0.0, 'ROUGE-L': 80.0, 'SacreBLEU': 31.95} (1.59s avg)
Checkpoint saved: 250 queries processed

============================================================
Batch 134/198: Processing 2 queries
============================================================
Predictions:  ['24', 'false']
Answer:  ['24', 'true']
Predictions:  ['24']
Answer:  ['24']
  [Batch] Query 2942: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.54s avg)
Predictions:  ['false']
Answer:  ['true']
  [Batch] Query 2943: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.54s avg)
Checkpoint saved: 252 queries processed

============================================================
Batch 135/198: Processing 2 queries
============================================================
Predictions:  ['solar cult labplas', '1']
Answer:  ['solar cult', '4']
Predictions:  ['solar cult labplas']
Answer:  ['solar cult']
  [Batch] Query 2944: {'F1': 80.0, 'EM': 0.0, 'ROUGE-L': 80.0, 'SacreBLEU': 0.0} (1.51s avg)
Predictions:  ['1']
Answer:  ['4']
  [Batch] Query 2945: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.51s avg)
Checkpoint saved: 254 queries processed

============================================================
Batch 136/198: Processing 2 queries
============================================================
Predictions:  ['750', 'ponderosa pine burn 1']
Answer:  ['816', 'mixed conifer burn 1']
Predictions:  ['750']
Answer:  ['816']
  [Batch] Query 2946: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.47s avg)
Predictions:  ['ponderosa pine burn 1']
Answer:  ['mixed conifer burn 1']
  [Batch] Query 2947: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 31.95} (1.47s avg)
Checkpoint saved: 256 queries processed

============================================================
Batch 137/198: Processing 2 queries
============================================================
Predictions:  ['q4 octdec', 'q4 716']
Answer:  Processing batches:  69%|██████▉   | 137/198 [26:10<09:18,  9.16s/it]Processing batches:  70%|██████▉   | 138/198 [26:16<08:06,  8.11s/it]Processing batches:  70%|███████   | 139/198 [26:20<06:58,  7.10s/it]Processing batches:  71%|███████   | 140/198 [26:24<05:48,  6.00s/it]Processing batches:  71%|███████   | 141/198 [26:28<05:08,  5.41s/it]Processing batches:  72%|███████▏  | 142/198 [26:33<04:58,  5.33s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 946, in process_batch_queries
    messages = build_messages(query, answer_format, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 815, in build_messages
    table_content = read_file(f'{file_path}/{query["FileName"]}.{FILE_EXTENSIONS[opt.format]}')
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/utils/common_util.py", line 38, in read_file
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
FileNotFoundError: [Errno 2] No such file or directory: '/data/pan/4xin/datasets/RealHiTBench/html/society-table10_swap.html'
Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 946, in process_batch_queries
    messages = build_messages(query, answer_format, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 815, in build_messages
    table_content = read_file(f'{file_path}/{query["FileName"]}.{FILE_EXTENSIONS[opt.format]}')
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/utils/common_util.py", line 38, in read_file
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
FileNotFoundError: [Errno 2] No such file or directory: '/data/pan/4xin/datasets/RealHiTBench/html/society-table10_swap.html'
Processing batches:  73%|███████▎  | 145/198 [26:37<02:41,  3.04s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1143, in forward
    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.75 GiB. GPU 0 has a total capacity of 79.25 GiB of which 23.93 GiB is free. Including non-PyTorch memory, this process has 55.32 GiB memory in use. Of the allocated memory 50.95 GiB is allocated by PyTorch, and 3.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  74%|███████▎  | 146/198 [26:45<03:36,  4.17s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1143, in forward
    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.75 GiB. GPU 0 has a total capacity of 79.25 GiB of which 23.93 GiB is free. Including non-PyTorch memory, this process has 55.32 GiB memory in use. Of the allocated memory 50.95 GiB is allocated by PyTorch, and 3.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  74%|███████▍  | 147/198 [26:49<03:33,  4.19s/it]Processing batches:  75%|███████▍  | 148/198 [26:55<03:52,  4.65s/it]Processing batches:  75%|███████▌  | 149/198 [27:00<03:43,  4.56s/it]Processing batches:  76%|███████▌  | 150/198 [27:04<03:37,  4.54s/it]['q1 janmar', 'q4']
Predictions:  ['q4 octdec']
Answer:  ['q1 janmar']
  [Batch] Query 2948: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.68s avg)
Predictions:  ['q4 716']
Answer:  ['q4']
  [Batch] Query 2949: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.68s avg)
Checkpoint saved: 258 queries processed

============================================================
Batch 138/198: Processing 2 queries
============================================================
Predictions:  ['2', 'q1 janmar 2021']
Answer:  ['3', 'q1 janmar 20210']
Predictions:  ['2']
Answer:  ['3']
  [Batch] Query 2950: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.71s avg)
Predictions:  ['q1 janmar 2021']
Answer:  ['q1 janmar 20210']
  [Batch] Query 2951: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.71s avg)
Checkpoint saved: 260 queries processed

============================================================
Batch 139/198: Processing 2 queries
============================================================
Predictions:  ['total exports total imports', '1008']
Answer:  ['total exports', '9262']
Predictions:  ['total exports total imports']
Answer:  ['total exports']
  [Batch] Query 2952: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 31.95} (2.26s avg)
Predictions:  ['1008']
Answer:  ['9262']
  [Batch] Query 2953: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.26s avg)
Checkpoint saved: 262 queries processed

============================================================
Batch 140/198: Processing 2 queries
============================================================
Predictions:  ['0', 'great britain england scotland wales']
Answer:  ['6142622', '3']
Predictions:  ['0']
Answer:  ['6142622']
  [Batch] Query 2954: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.59s avg)
Predictions:  ['great britain england scotland wales']
Answer:  ['3']
  [Batch] Query 2955: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.59s avg)
Checkpoint saved: 264 queries processed

============================================================
Batch 141/198: Processing 2 queries
============================================================
Predictions:  ['chile norway', 'canada']
Answer:  ['norway', 'mexico']
Predictions:  ['chile norway']
Answer:  ['norway']
  [Batch] Query 2956: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (1.90s avg)
Predictions:  ['canada']
Answer:  ['mexico']
  [Batch] Query 2957: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.90s avg)
Checkpoint saved: 266 queries processed

============================================================
Batch 142/198: Processing 2 queries
============================================================
Predictions:  ['2017 2018', 'india indonesia ecuador thailand vietnam mexico peru other countries']
Answer:  ['2016 2017', 'india indonesia thailand vietnam ecuador mexico china mainland']
Predictions:  ['2017 2018']
Answer:  ['2016 2017']
  [Batch] Query 2958: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (2.45s avg)
Predictions:  ['india indonesia ecuador thailand vietnam mexico peru other countries']
Answer:  ['india indonesia thailand vietnam ecuador mexico china mainland']
  [Batch] Query 2959: {'F1': 70.59, 'EM': 0.0, 'ROUGE-L': 58.82, 'SacreBLEU': 14.92} (2.45s avg)
Checkpoint saved: 268 queries processed

============================================================
Batch 143/198: Processing 2 queries
============================================================
Error processing batch 143: [Errno 2] No such file or directory: '/data/pan/4xin/datasets/RealHiTBench/html/society-table10_swap.html'

============================================================
Batch 144/198: Processing 2 queries
============================================================
Error processing batch 144: [Errno 2] No such file or directory: '/data/pan/4xin/datasets/RealHiTBench/html/society-table10_swap.html'

============================================================
Batch 145/198: Processing 2 queries
============================================================
Predictions:  ['bituminous 100', '01']
Answer:  ['dark roast biomass 40', '01']
Predictions:  ['bituminous 100']
Answer:  ['dark roast biomass 40']
  [Batch] Query 2964: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.77s avg)
Predictions:  ['01']
Answer:  ['01']
  [Batch] Query 2965: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.77s avg)
Checkpoint saved: 270 queries processed

============================================================
Batch 146/198: Processing 2 queries
============================================================
Error processing batch 146: CUDA out of memory. Tried to allocate 68.75 GiB. GPU 0 has a total capacity of 79.25 GiB of which 23.93 GiB is free. Including non-PyTorch memory, this process has 55.32 GiB memory in use. Of the allocated memory 50.95 GiB is allocated by PyTorch, and 3.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 147/198: Processing 2 queries
============================================================
Error processing batch 147: CUDA out of memory. Tried to allocate 68.75 GiB. GPU 0 has a total capacity of 79.25 GiB of which 23.93 GiB is free. Including non-PyTorch memory, this process has 55.32 GiB memory in use. Of the allocated memory 50.95 GiB is allocated by PyTorch, and 3.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 148/198: Processing 2 queries
============================================================
Predictions:  ['netfosaa', '62 fts 63 ftca 73 ftca 53 ftca 82 ftca 102 ftuca']
Answer:  ['netfosaa', '3']
Predictions:  ['netfosaa']
Answer:  ['netfosaa']
  [Batch] Query 2970: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.90s avg)
Predictions:  ['62 fts 63 ftca 73 ftca 53 ftca 82 ftca 102 ftuca']
Answer:  ['3']
  [Batch] Query 2971: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.90s avg)
Checkpoint saved: 272 queries processed

============================================================
Batch 149/198: Processing 2 queries
============================================================
Predictions:  ['pfba pfda', 'no data available']
Answer:  ['pfda', '2013 fall']
Predictions:  ['pfba pfda']
Answer:  ['pfda']
  [Batch] Query 2972: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.04s avg)
Predictions:  ['no data available']
Answer:  ['2013 fall']
  [Batch] Query 2973: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.04s avg)
Checkpoint saved: 274 queries processed

============================================================
Batch 150/198: Processing 2 queries
============================================================
Predictions:  ['0 0', '0']
Answer:  ['15', '1']
Predictions:  ['0 0']
Answer:  ['15']
  [Batch] Query 2974: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.12s avg)
Predictions:  ['0']
Answer:  ['1']
  [Batch] Query 2975: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.12s avg)
Checkpoint saved: 276 queries processed

============================================================
Batch 151/198: Processing 2 queries
============================================================
Predictions:  ['pennsylvania ohio', '1161467']
Answer:  ['pennsylvania', '1161467']
Predictions:  ['pennsylvania ohio']
Answer:  ['pennsylvania']
  [Batch] Query 2976: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.80s avg)
Predictions:  ['1161467']
Answer:  ['1161467']
Processing batches:  76%|███████▋  | 151/198 [27:10<03:50,  4.91s/it]Processing batches:  77%|███████▋  | 152/198 [27:16<03:59,  5.20s/it]Processing batches:  77%|███████▋  | 153/198 [27:28<05:23,  7.18s/it]Processing batches:  78%|███████▊  | 154/198 [27:48<08:03, 10.98s/it]Processing batches:  78%|███████▊  | 155/198 [27:52<06:22,  8.91s/it]Processing batches:  79%|███████▉  | 156/198 [27:59<05:50,  8.34s/it]Processing batches:  79%|███████▉  | 157/198 [28:06<05:24,  7.92s/it]Processing batches:  80%|███████▉  | 158/198 [28:12<04:54,  7.35s/it]Processing batches:  80%|████████  | 159/198 [28:15<03:59,  6.14s/it]Processing batches:  81%|████████  | 160/198 [28:20<03:39,  5.79s/it]Processing batches:  81%|████████▏ | 161/198 [28:24<03:15,  5.29s/it]Processing batches:  82%|████████▏ | 162/198 [28:28<02:56,  4.91s/it]Processing batches:  82%|████████▏ | 163/198 [28:32<02:42,  4.65s/it]Processing batches:  83%|████████▎ | 164/198 [28:35<02:18,  4.08s/it]  [Batch] Query 2977: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.80s avg)
Checkpoint saved: 278 queries processed

============================================================
Batch 152/198: Processing 2 queries
============================================================
Predictions:  ['illinois 115711', 'no']
Answer:  ['illinois 7007', 'no']
Predictions:  ['illinois 115711']
Answer:  ['illinois 7007']
  [Batch] Query 2978: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (2.85s avg)
Predictions:  ['no']
Answer:  ['no']
  [Batch] Query 2979: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.85s avg)
Checkpoint saved: 280 queries processed

============================================================
Batch 153/198: Processing 2 queries
============================================================
Predictions:  ['mississippi 246', 'florida 457']
Answer:  ['mississippi 246', 'south carolina 262']
Predictions:  ['mississippi 246']
Answer:  ['mississippi 246']
  [Batch] Query 2980: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (5.89s avg)
Predictions:  ['florida 457']
Answer:  ['south carolina 262']
  [Batch] Query 2981: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (5.89s avg)
Checkpoint saved: 282 queries processed

============================================================
Batch 154/198: Processing 2 queries
============================================================
Predictions:  ['texas california new york pennsylvania ohio michigan illinois florida georgia north carolina massachusetts new jersey virginia washington arizona indiana kentucky missouri tennessee wisconsin alabama arkansas louisiana mississippi south carolina south dakota west virginia wyoming oregon connecticut delaware maryland hawaii idaho iowa kansas nebraska nevada new hampshire oklahoma rhode island utah vermont district of columbia', '246']
Answer:  ['4', '185']
Predictions:  ['texas california new york pennsylvania ohio michigan illinois florida georgia north carolina massachusetts new jersey virginia washington arizona indiana kentucky missouri tennessee wisconsin alabama arkansas louisiana mississippi south carolina south dakota west virginia wyoming oregon connecticut delaware maryland hawaii idaho iowa kansas nebraska nevada new hampshire oklahoma rhode island utah vermont district of columbia']
Answer:  ['4']
  [Batch] Query 2982: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (9.96s avg)
Predictions:  ['246']
Answer:  ['185']
  [Batch] Query 2983: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (9.96s avg)
Checkpoint saved: 284 queries processed

============================================================
Batch 155/198: Processing 2 queries
============================================================
Predictions:  ['16 to 24 years', '586']
Answer:  ['20 to 24 years', '586']
Predictions:  ['16 to 24 years']
Answer:  ['20 to 24 years']
  [Batch] Query 2984: {'F1': 75.0, 'EM': 0.0, 'ROUGE-L': 75.0, 'SacreBLEU': 59.46} (1.86s avg)
Predictions:  ['586']
Answer:  ['586']
  [Batch] Query 2985: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.86s avg)
Checkpoint saved: 286 queries processed

============================================================
Batch 156/198: Processing 2 queries
============================================================
Predictions:  ['274 274', 'aircraft pilots and flight engineers']
Answer:  ['249', 'chief executives']
Predictions:  ['274 274']
Answer:  ['249']
  [Batch] Query 2986: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.37s avg)
Predictions:  ['aircraft pilots and flight engineers']
Answer:  ['chief executives']
  [Batch] Query 2987: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.37s avg)
Checkpoint saved: 288 queries processed

============================================================
Batch 157/198: Processing 2 queries
============================================================
Predictions:  ['24', '63']
Answer:  ['30', '72']
Predictions:  ['24']
Answer:  ['30']
  [Batch] Query 2988: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.35s avg)
Predictions:  ['63']
Answer:  ['72']
  [Batch] Query 2989: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.35s avg)
Checkpoint saved: 290 queries processed

============================================================
Batch 158/198: Processing 2 queries
============================================================
Predictions:  ['dishwashers', 'no']
Answer:  ['insurance sales agents', 'no']
Predictions:  ['dishwashers']
Answer:  ['insurance sales agents']
  [Batch] Query 2990: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.88s avg)
Predictions:  ['no']
Answer:  ['no']
  [Batch] Query 2991: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.88s avg)
Checkpoint saved: 292 queries processed

============================================================
Batch 159/198: Processing 2 queries
============================================================
Predictions:  ['5813', '25 to 34 years']
Answer:  ['8932', '25 to 34 years']
Predictions:  ['5813']
Answer:  ['8932']
  [Batch] Query 2992: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.52s avg)
Predictions:  ['25 to 34 years']
Answer:  ['25 to 34 years']
  [Batch] Query 2993: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (1.52s avg)
Checkpoint saved: 294 queries processed

============================================================
Batch 160/198: Processing 2 queries
============================================================
Predictions:  ['41', '13564 13564 13564']
Answer:  ['10', '14108']
Predictions:  ['41']
Answer:  ['10']
  [Batch] Query 2994: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.36s avg)
Predictions:  ['13564 13564 13564']
Answer:  ['14108']
  [Batch] Query 2995: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.36s avg)
Checkpoint saved: 296 queries processed

============================================================
Batch 161/198: Processing 2 queries
============================================================
Predictions:  ['1027', '04']
Answer:  ['949', '343']
Predictions:  ['1027']
Answer:  ['949']
  [Batch] Query 2996: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.95s avg)
Predictions:  ['04']
Answer:  ['343']
  [Batch] Query 2997: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.95s avg)
Checkpoint saved: 298 queries processed

============================================================
Batch 162/198: Processing 2 queries
============================================================
Predictions:  ['234', 'no']
Answer:  ['254', 'no']
Predictions:  ['234']
Answer:  ['254']
  [Batch] Query 2998: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.89s avg)
Predictions:  ['no']
Answer:  ['no']
  [Batch] Query 2999: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.89s avg)
Checkpoint saved: 300 queries processed

============================================================
Batch 163/198: Processing 2 queries
============================================================
Predictions:  ['18', '16 to 24 years']
Answer:  ['79', '16 to 24 years']
Predictions:  ['18']
Answer:  ['79']
  [Batch] Query 3000: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.90s avg)
Predictions:  ['16 to 24 years']
Answer:  ['16 to 24 years']
  [Batch] Query 3001: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (1.90s avg)
Checkpoint saved: 302 queries processed

============================================================
Batch 164/198: Processing 2 queries
============================================================
Predictions:  ['69', 'yes']
Answer:  ['69', 'no']
Predictions:  ['69']
Answer:  ['69']
  [Batch] Query 3002: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.25s avg)
Predictions:  ['yes']
Answer:  ['no']
  [Batch] Query 3003: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.25s avg)
Checkpoint saved: 304 queries processed

============================================================
Processing batches:  83%|████████▎ | 165/198 [28:39<02:14,  4.08s/it]Processing batches:  84%|████████▍ | 166/198 [28:44<02:12,  4.15s/it]Processing batches:  84%|████████▍ | 167/198 [28:51<02:40,  5.19s/it]Processing batches:  85%|████████▍ | 168/198 [29:04<03:46,  7.54s/it]Processing batches:  85%|████████▌ | 169/198 [29:19<04:44,  9.81s/it]Processing batches:  86%|████████▌ | 170/198 [29:34<05:17, 11.35s/it]Processing batches:  86%|████████▋ | 171/198 [29:43<04:44, 10.52s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1143, in forward
    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.28 GiB. GPU 0 has a total capacity of 79.25 GiB of which 23.93 GiB is free. Including non-PyTorch memory, this process has 55.32 GiB memory in use. Of the allocated memory 40.70 GiB is allocated by PyTorch, and 14.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  87%|████████▋ | 172/198 [29:46<03:39,  8.44s/it]Traceback (most recent call last):
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 1186, in gen_solution_batch
    batch_results = process_batch_queries(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 955, in process_batch_queries
    batch_responses = get_batch_final_answers_local(
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 593, in get_batch_final_answers_local
    responses = get_batch_image_response_local(pending_messages, pending_image_files, model, processor, opt)
  File "/export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py", line 424, in get_batch_image_response_local
    generated_ids = base_model.generate(**generate_kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2566, in generate
    result = decoding_method(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/generation/utils.py", line 2786, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1344, in forward
    outputs = self.model(
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/utils/generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/export/home/pan/miniconda3/envs/4xin-hit/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py", line 1143, in forward
    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.28 GiB. GPU 0 has a total capacity of 79.25 GiB of which 23.93 GiB is free. Including non-PyTorch memory, this process has 55.32 GiB memory in use. Of the allocated memory 40.70 GiB is allocated by PyTorch, and 14.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing batches:  87%|████████▋ | 173/198 [29:52<03:07,  7.52s/it]Processing batches:  88%|████████▊ | 174/198 [30:25<06:05, 15.22s/it]Processing batches:  88%|████████▊ | 175/198 [30:48<06:43, 17.56s/it]Processing batches:  89%|████████▉ | 176/198 [30:55<05:13, 14.27s/it]Processing batches:  89%|████████▉ | 177/198 [30:58<03:50, 10.99s/it]Batch 165/198: Processing 2 queries
============================================================
Predictions:  ['2015', '2012']
Answer:  ['2016', '2015']
Predictions:  ['2015']
Answer:  ['2016']
  [Batch] Query 3004: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.92s avg)
Predictions:  ['2012']
Answer:  ['2015']
  [Batch] Query 3005: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.92s avg)
Checkpoint saved: 306 queries processed

============================================================
Batch 166/198: Processing 2 queries
============================================================
Predictions:  ['4', '83']
Answer:  ['5', '82']
Predictions:  ['4']
Answer:  ['5']
  [Batch] Query 3006: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.04s avg)
Predictions:  ['83']
Answer:  ['82']
  [Batch] Query 3007: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.04s avg)
Checkpoint saved: 308 queries processed

============================================================
Batch 167/198: Processing 2 queries
============================================================
Predictions:  ['288321', 'no']
Answer:  ['288321', 'no']
Predictions:  ['288321']
Answer:  ['288321']
  [Batch] Query 3008: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (3.68s avg)
Predictions:  ['no']
Answer:  ['no']
  [Batch] Query 3009: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (3.68s avg)
Checkpoint saved: 310 queries processed

============================================================
Batch 168/198: Processing 2 queries
============================================================
Predictions:  ['327200 330200', 'not available']
Answer:  ['increase', '793']
Predictions:  ['327200 330200']
Answer:  ['increase']
  [Batch] Query 3010: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (6.39s avg)
Predictions:  ['not available']
Answer:  ['793']
  [Batch] Query 3011: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (6.39s avg)
Checkpoint saved: 312 queries processed

============================================================
Batch 169/198: Processing 2 queries
============================================================
Predictions:  ['4990', '36894 641']
Answer:  ['2017', '368936 6409 362527']
Predictions:  ['4990']
Answer:  ['2017']
  [Batch] Query 3012: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (7.44s avg)
Predictions:  ['36894 641']
Answer:  ['368936 6409 362527']
  [Batch] Query 3013: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (7.44s avg)
Checkpoint saved: 314 queries processed

============================================================
Batch 170/198: Processing 2 queries
============================================================
Predictions:  ['no', '1007 yes']
Answer:  ['yes', '1022 yes']
Predictions:  ['no']
Answer:  ['yes']
  [Batch] Query 3014: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (7.35s avg)
Predictions:  ['1007 yes']
Answer:  ['1022 yes']
  [Batch] Query 3015: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (7.35s avg)
Checkpoint saved: 316 queries processed

============================================================
Batch 171/198: Processing 2 queries
============================================================
Predictions:  ['yes', '2256938']
Answer:  ['yes', '2256938']
Predictions:  ['yes']
Answer:  ['yes']
  [Batch] Query 3016: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (4.18s avg)
Predictions:  ['2256938']
Answer:  ['2256938']
  [Batch] Query 3017: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (4.18s avg)
Checkpoint saved: 318 queries processed

============================================================
Batch 172/198: Processing 2 queries
============================================================
Error processing batch 172: CUDA out of memory. Tried to allocate 48.28 GiB. GPU 0 has a total capacity of 79.25 GiB of which 23.93 GiB is free. Including non-PyTorch memory, this process has 55.32 GiB memory in use. Of the allocated memory 40.70 GiB is allocated by PyTorch, and 14.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 173/198: Processing 2 queries
============================================================
Error processing batch 173: CUDA out of memory. Tried to allocate 48.28 GiB. GPU 0 has a total capacity of 79.25 GiB of which 23.93 GiB is free. Including non-PyTorch memory, this process has 55.32 GiB memory in use. Of the allocated memory 40.70 GiB is allocated by PyTorch, and 14.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Batch 174/198: Processing 2 queries
============================================================
Predictions:  ['111987 111987', '1234463']
Answer:  ['74883', '342027']
Predictions:  ['111987 111987']
Answer:  ['74883']
  [Batch] Query 3022: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (16.47s avg)
Predictions:  ['1234463']
Answer:  ['342027']
  [Batch] Query 3023: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (16.47s avg)
Checkpoint saved: 320 queries processed

============================================================
Batch 175/198: Processing 2 queries
============================================================
Predictions:  ['1234463', '36188']
Answer:  ['439317', '36188']
Predictions:  ['1234463']
Answer:  ['439317']
  [Batch] Query 3024: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (11.39s avg)
Predictions:  ['36188']
Answer:  ['36188']
  [Batch] Query 3025: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (11.39s avg)
Checkpoint saved: 322 queries processed

============================================================
Batch 176/198: Processing 2 queries
============================================================
Predictions:  ['493143', '36188 0']
Answer:  ['493143', '529331']
Predictions:  ['493143']
Answer:  ['493143']
  [Batch] Query 3026: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (3.17s avg)
Predictions:  ['36188 0']
Answer:  ['529331']
  [Batch] Query 3027: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.17s avg)
Checkpoint saved: 324 queries processed

============================================================
Batch 177/198: Processing 2 queries
============================================================
Predictions:  ['calculus linear algebra materials science', 'accounting afrikaans 1st add language computer applic technology life orientation mathematics physical sciences']
Answer:  ['yes', 'chemistry mechanics materials science workshop practice accounting afrikaans 1st add language computer application technology life orientation mathematics physical sciences']
Predictions:  ['calculus linear algebra materials science']
Answer:  ['yes']
  [Batch] Query 3028: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.54s avg)
Predictions:  ['accounting afrikaans 1st add language computer applic technology life orientation mathematics physical sciences']
Answer:  ['chemistry mechanics materials science workshop practice accounting afrikaans 1st add language computer application technology life orientation mathematics physical sciences']
  [Batch] Query 3029: {'F1': 75.0, 'EM': 0.0, 'ROUGE-L': 75.0, 'SacreBLEU': 47.98} (1.54s avg)
Checkpoint saved: 326 queries processed

============================================================
Batch 178/198: Processing 2 queries
============================================================
Predictions:  ['572 424', '10 745']
Answer:  ['629 472', '10 629']
Predictions:  ['572 424']
Answer:  Processing batches:  90%|████████▉ | 178/198 [31:00<02:48,  8.43s/it]Processing batches:  90%|█████████ | 179/198 [31:04<02:10,  6.86s/it]Processing batches:  91%|█████████ | 180/198 [31:07<01:46,  5.92s/it]Processing batches:  91%|█████████▏| 181/198 [31:10<01:22,  4.88s/it]Processing batches:  92%|█████████▏| 182/198 [31:13<01:10,  4.38s/it]Processing batches:  92%|█████████▏| 183/198 [31:16<00:58,  3.90s/it]Processing batches:  93%|█████████▎| 184/198 [31:18<00:49,  3.55s/it]Processing batches:  93%|█████████▎| 185/198 [31:22<00:44,  3.43s/it]Processing batches:  94%|█████████▍| 186/198 [31:25<00:39,  3.28s/it]Processing batches:  94%|█████████▍| 187/198 [31:32<00:49,  4.51s/it]Processing batches:  95%|█████████▍| 188/198 [31:39<00:52,  5.22s/it]Processing batches:  95%|█████████▌| 189/198 [31:43<00:43,  4.83s/it]Processing batches:  96%|█████████▌| 190/198 [31:48<00:39,  4.88s/it]Processing batches:  96%|█████████▋| 191/198 [31:50<00:29,  4.21s/it]['629 472']
  [Batch] Query 3030: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.10s avg)
Predictions:  ['10 745']
Answer:  ['10 629']
  [Batch] Query 3031: {'F1': 50.0, 'EM': 0.0, 'ROUGE-L': 50.0, 'SacreBLEU': 0.0} (1.10s avg)
Checkpoint saved: 328 queries processed

============================================================
Batch 179/198: Processing 2 queries
============================================================
Predictions:  ['register', 'mobile department']
Answer:  ['register', 'mobile department']
Predictions:  ['register']
Answer:  ['register']
  [Batch] Query 3032: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.49s avg)
Predictions:  ['mobile department']
Answer:  ['mobile department']
  [Batch] Query 3033: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.49s avg)
Checkpoint saved: 330 queries processed

============================================================
Batch 180/198: Processing 2 queries
============================================================
Predictions:  ['mobile department register other department register', '139 1065']
Answer:  ['mobile department register', '141']
Predictions:  ['mobile department register other department register']
Answer:  ['mobile department register']
  [Batch] Query 3034: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 30.21} (1.74s avg)
Predictions:  ['139 1065']
Answer:  ['141']
  [Batch] Query 3035: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.74s avg)
Checkpoint saved: 332 queries processed

============================================================
Batch 181/198: Processing 2 queries
============================================================
Predictions:  ['spain', '1']
Answer:  ['spain', '1']
Predictions:  ['spain']
Answer:  ['spain']
  [Batch] Query 3036: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.10s avg)
Predictions:  ['1']
Answer:  ['1']
  [Batch] Query 3037: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.10s avg)
Checkpoint saved: 334 queries processed

============================================================
Batch 182/198: Processing 2 queries
============================================================
Predictions:  ['finance hr it legal sales travel', '15']
Answer:  ['finance hr it legal sales travel', '16']
Predictions:  ['finance hr it legal sales travel']
Answer:  ['finance hr it legal sales travel']
  [Batch] Query 3038: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 100.0} (1.49s avg)
Predictions:  ['15']
Answer:  ['16']
  [Batch] Query 3039: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.49s avg)
Checkpoint saved: 336 queries processed

============================================================
Batch 183/198: Processing 2 queries
============================================================
Predictions:  ['16', 'control tbooh']
Answer:  ['16', 'tbooh in control']
Predictions:  ['16']
Answer:  ['16']
  [Batch] Query 3040: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.27s avg)
Predictions:  ['control tbooh']
Answer:  ['tbooh in control']
  [Batch] Query 3041: {'F1': 80.0, 'EM': 0.0, 'ROUGE-L': 40.0, 'SacreBLEU': 0.0} (1.27s avg)
Checkpoint saved: 338 queries processed

============================================================
Batch 184/198: Processing 2 queries
============================================================
Predictions:  ['01', 'control']
Answer:  ['01', 'after hd']
Predictions:  ['01']
Answer:  ['01']
  [Batch] Query 3042: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.24s avg)
Predictions:  ['control']
Answer:  ['after hd']
  [Batch] Query 3043: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.24s avg)
Checkpoint saved: 340 queries processed

============================================================
Batch 185/198: Processing 2 queries
============================================================
Predictions:  ['03', 'base']
Answer:  ['03', 'tbooh']
Predictions:  ['03']
Answer:  ['03']
  [Batch] Query 3044: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.46s avg)
Predictions:  ['base']
Answer:  ['tbooh']
  [Batch] Query 3045: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.46s avg)
Checkpoint saved: 342 queries processed

============================================================
Batch 186/198: Processing 2 queries
============================================================
Predictions:  ['00', 'before hd tbooh']
Answer:  ['00', 'before hd']
Predictions:  ['00']
Answer:  ['00']
  [Batch] Query 3046: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.35s avg)
Predictions:  ['before hd tbooh']
Answer:  ['before hd']
  [Batch] Query 3047: {'F1': 80.0, 'EM': 0.0, 'ROUGE-L': 80.0, 'SacreBLEU': 0.0} (1.35s avg)
Checkpoint saved: 344 queries processed

============================================================
Batch 187/198: Processing 2 queries
============================================================
Predictions:  ['republic of korea', '16']
Answer:  ['republic of korea', '16']
Predictions:  ['republic of korea']
Answer:  ['republic of korea']
  [Batch] Query 3048: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (3.57s avg)
Predictions:  ['16']
Answer:  ['16']
  [Batch] Query 3049: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (3.57s avg)
Checkpoint saved: 346 queries processed

============================================================
Batch 188/198: Processing 2 queries
============================================================
Predictions:  ['croatia', 'japan']
Answer:  ['bulgaria', 'singapore']
Predictions:  ['croatia']
Answer:  ['bulgaria']
  [Batch] Query 3050: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.31s avg)
Predictions:  ['japan']
Answer:  ['singapore']
  [Batch] Query 3051: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (3.31s avg)
Checkpoint saved: 348 queries processed

============================================================
Batch 189/198: Processing 2 queries
============================================================
Predictions:  ['greece', '81']
Answer:  ['republic of korea', '2584']
Predictions:  ['greece']
Answer:  ['republic of korea']
  [Batch] Query 3052: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.84s avg)
Predictions:  ['81']
Answer:  ['2584']
  [Batch] Query 3053: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.84s avg)
Checkpoint saved: 350 queries processed

============================================================
Batch 190/198: Processing 2 queries
============================================================
Predictions:  ['japan', 'bulgaria czech republic hungary poland republic of moldova romania russian federation ukraine']
Answer:  ['japan', '5']
Predictions:  ['japan']
Answer:  ['japan']
  [Batch] Query 3054: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.38s avg)
Predictions:  ['bulgaria czech republic hungary poland republic of moldova romania russian federation ukraine']
Answer:  ['5']
  [Batch] Query 3055: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.38s avg)
Checkpoint saved: 352 queries processed

============================================================
Batch 191/198: Processing 2 queries
============================================================
Predictions:  ['321', 'champion reefs']
Answer:  ['321', 'champion reefs']
Predictions:  ['321']
Answer:  ['321']
  [Batch] Query 3056: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.21s avg)
Predictions:  ['champion reefs']
Answer:  ['champion reefs']
  [Batch] Query 3057: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.21s avg)
Checkpoint saved: 354 queries processed

============================================================
Batch 192/198: Processing 2 queries
============================================================
Predictions:  ['17', 'champion reefs']
Answer:  ['17', 'oorgaum']
Predictions:  ['17']
Answer:  ['17']
  [Batch] Query 3058: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.20s avg)
Predictions:  Processing batches:  97%|█████████▋| 192/198 [31:53<00:22,  3.74s/it]Processing batches:  97%|█████████▋| 193/198 [31:59<00:21,  4.34s/it]Processing batches:  98%|█████████▊| 194/198 [32:04<00:18,  4.67s/it]Processing batches:  98%|█████████▊| 195/198 [32:08<00:13,  4.40s/it]Processing batches:  99%|█████████▉| 196/198 [32:12<00:08,  4.18s/it]Processing batches:  99%|█████████▉| 197/198 [32:15<00:03,  3.95s/it]Processing batches: 100%|██████████| 198/198 [32:19<00:00,  3.82s/it]Processing batches: 100%|██████████| 198/198 [32:19<00:00,  9.79s/it]
['champion reefs']
Answer:  ['oorgaum']
  [Batch] Query 3059: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.20s avg)
Checkpoint saved: 356 queries processed

============================================================
Batch 193/198: Processing 2 queries
============================================================
Predictions:  ['226', '157 157']
Answer:  ['226', '157']
Predictions:  ['226']
Answer:  ['226']
  [Batch] Query 3060: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (2.75s avg)
Predictions:  ['157 157']
Answer:  ['157']
  [Batch] Query 3061: {'F1': 66.67, 'EM': 0.0, 'ROUGE-L': 66.67, 'SacreBLEU': 0.0} (2.75s avg)
Checkpoint saved: 358 queries processed

============================================================
Batch 194/198: Processing 2 queries
============================================================
Predictions:  ['228', 'set5 156']
Answer:  ['231', 'set1 155']
Predictions:  ['228']
Answer:  ['231']
  [Batch] Query 3062: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.60s avg)
Predictions:  ['set5 156']
Answer:  ['set1 155']
  [Batch] Query 3063: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (2.60s avg)
Checkpoint saved: 360 queries processed

============================================================
Batch 195/198: Processing 2 queries
============================================================
Predictions:  ['24738', 'set6']
Answer:  ['24425', 'set6']
Predictions:  ['24738']
Answer:  ['24425']
  [Batch] Query 3064: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.76s avg)
Predictions:  ['set6']
Answer:  ['set6']
  [Batch] Query 3065: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.76s avg)
Checkpoint saved: 362 queries processed

============================================================
Batch 196/198: Processing 2 queries
============================================================
Predictions:  ['1383', 'expt2']
Answer:  ['1288', 'expt2']
Predictions:  ['1383']
Answer:  ['1288']
  [Batch] Query 3066: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.72s avg)
Predictions:  ['expt2']
Answer:  ['expt2']
  [Batch] Query 3067: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.72s avg)
Checkpoint saved: 364 queries processed

============================================================
Batch 197/198: Processing 2 queries
============================================================
Predictions:  ['342', 'set 3']
Answer:  ['342', 'set 3']
Predictions:  ['342']
Answer:  ['342']
  [Batch] Query 3068: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.58s avg)
Predictions:  ['set 3']
Answer:  ['set 3']
  [Batch] Query 3069: {'F1': 100.0, 'EM': 100.0, 'ROUGE-L': 100.0, 'SacreBLEU': 0.0} (1.58s avg)
Checkpoint saved: 366 queries processed

============================================================
Batch 198/198: Processing 2 queries
============================================================
Predictions:  ['15', '25899']
Answer:  ['05', '19609']
Predictions:  ['15']
Answer:  ['05']
  [Batch] Query 3070: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.64s avg)
Predictions:  ['25899']
Answer:  ['19609']
  [Batch] Query 3071: {'F1': 0.0, 'EM': 0.0, 'ROUGE-L': 0.0, 'SacreBLEU': 0.0} (1.64s avg)
Checkpoint saved: 368 queries processed

============================================================
BATCH EVALUATION COMPLETE
============================================================
Total queries: 368
Duration: 1950.49s
Throughput: 0.19 queries/sec
Results saved to: /export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/result/qwen3vl_local_a100/Qwen3-VL-8B-Instruct_mix_html_a100/results_batch_20260131_000613.json

Aggregate Metrics by Question Type:
  Structure Comprehending:
    EM: 35.0543
    ROUGE-L: 48.1620
    F1: 49.1863
    SacreBLEU: 5.5221
Checkpoint preserved at: /export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/result/qwen3vl_local_a100/Qwen3-VL-8B-Instruct_mix_html_a100/checkpoint_batch.json
Running Structure Comprehending inference for mix_html...
Command: /export/home/pan/miniconda3/envs/4xin-hit/bin/python /export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/inference/inference_qwen3vl_local_a100.py --modality mix --format html --model_dir /data/pan/4xin/models/Qwen3-VL-8B-Instruct --data_path /data/pan/4xin/datasets/RealHiTBench --qa_path /export/home/pan/4xin/RealHiTBENCH-Qwen3-VL/data --question_type Structure Comprehending --batch_size 2 --use_sc_filled
--------------------------------------------------------------------------------

✓ Successfully completed mix_html inference
